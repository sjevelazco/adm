% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fit_abund_dnn.R
\name{fit_abund_dnn}
\alias{fit_abund_dnn}
\title{Fit and validate Deep Neural Network model}
\usage{
fit_abund_dnn(
  data,
  response,
  predictors,
  predictors_f = NULL,
  partition,
  predict_part = FALSE,
  learning_rate = 0.01,
  weight_decay = 0,
  n_epochs = 10,
  batch_size = 32,
  validation_patience = 2,
  fitting_patience = 5,
  optimizer = torch::optim_adamw,
  loss_function = torch::nn_l1_loss,
  custom_architecture = NULL,
  verbose = TRUE,
  learning_monitor = FALSE
)
}
\arguments{
\item{data}{tibble or data.frame. Database with response, predictors, and partition values}

\item{response}{character. Column name with species abundance.}

\item{predictors}{character. Vector with the column names of quantitative predictor variables (i.e. continuous variables). Usage predictors = c("temp", "precipt", "sand")}

\item{predictors_f}{character. Vector with the column names of qualitative predictor variables (i.e. ordinal or nominal variables type). Usage predictors_f = c("landform")}

\item{partition}{character. Column name with training and validation partition groups.}

\item{predict_part}{logical. Save predicted abundance for testing data. Default = FALSE}

\item{learning_rate}{numeric. The size of the step taken during the optimization process. Default = 0.01}

\item{weight_decay}{numeric. The regularization strength: 0 means no penalty, while higher values (e.g. 0.01) apply stronger shrinkage to the weights during training. Default is 0}

\item{n_epochs}{numeric. Max number of times the learning algorithm will work through the training set. Default = 10}

\item{batch_size}{numeric. A batch is a subset of the training set used in a single iteration of the training process. The size of each batch is referred to as the batch size. Default = 32}

\item{validation_patience}{numerical. An integer indicating the number of epochs without loss improvement tolerated by the algorithm in the validation process. If the patience limit is exceeded, the training ends. Default 2}

\item{fitting_patience}{numerical. The same as validation_patience, but in the final model fitting process. Default 5}

\item{optimizer}{a torch_optimizer_generator. The optimizer to be used in model fitting. Default is torch::optim_adamw.}

\item{loss_function}{a torch nn_loss. The loss function to be used in model fitting. Default is torch::nn_l1_loss.}

\item{custom_architecture}{a Torch nn_module_generator object or a generate_dnn_architecture output. A neural network architecture to be used instead of the internal default one. Default NULL}

\item{verbose}{logical. If FALSE, disables all console messages. Default TRUE}

\item{learning_monitor}{logical. If TRUE, the function will return a tibble containing loss values over training epochs. It is useful to create learning/convergence plots.}
}
\value{
A list object with:
\itemize{
\item model: A "luz_module_fitted" object from luz (torch framework). This object can be used to predicting.
\item predictors: A tibble with quantitative (c column names) and qualitative (f column names) variables use for modeling.
\item performance: Averaged performance metrics (see \code{\link{adm_eval}}).
\item performance_part: Performance metrics for each replica and partition.
\item predicted_part: Observed and predicted abundance for each test partition.
}
}
\description{
Fit and validate Deep Neural Network model
}
\examples{
\dontrun{
require(dplyr)

# Database with species abundance and x and y coordinates
data("sppabund")

# Extract data for a single species
some_sp <- sppabund \%>\%
  dplyr::filter(species == "Species one") \%>\%
  dplyr::select(-.part2, -.part3)

# Explore reponse variables
some_sp$ind_ha \%>\% range()
some_sp$ind_ha \%>\% hist()

# Here we balance number of absences
some_sp <-
  balance_dataset(some_sp, response = "ind_ha", absence_ratio = 0.2)

# Generate a architecture
dnn_arch <- generate_dnn_architecture(
  number_of_features = 3,
  number_of_outputs = 1,
  number_of_hidden_layers = 3,
  hidden_layers_size = c(8, 16, 8),
  batch_norm = TRUE
)

# Fit a NET model
mdnn <- fit_abund_dnn(
  data = some_sp,
  response = "ind_ha",
  predictors = c("bio12", "elevation", "sand"),
  predictors_f = NULL,
  partition = ".part",
  learning_rate = 0.01,
  n_epochs = 10,
  batch_size = 32,
  validation_patience = 2,
  fitting_patience = 5,
  custom_architecture = dnn_arch,
  verbose = TRUE,
  predict_part = TRUE
)

mdnn
}
}
