[{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Hyperparameter tuning in adm","text":"Using multiple algorithms performing hyperparameters tuning crucial diversifying species abundance modeling. Functions tune_abund prefix allow users easily perform model tuning different algorithms. tuning performed using grid-search approach, .e., evaluating model performance many hyperparameter combinations. , user provides simple data frame hyperparameters columns hyperparameter values tested rows. vignette, show construct use grid give details hyperparameters can tuned algorithm.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"which-hyperparameters-can-i-tune","dir":"Articles","previous_headings":"","what":"Which hyperparameters can I tune?","title":"Hyperparameter tuning in adm","text":"adm features nine algorithms, set hyperparameters. user can choose hyperparameters tune values test one. tuning process done grid-search approach. grid data frame hyperparameters columns hyperparameter values tested rows. See list algorithms hyperparameters. size decay Convolutional Neural Networks (_cnn) Deep Neural Networks (_dnn)* learning_rate n_epochs batch_size validation_patience fitting_patience nrounds max_depth eta gamma colsample_bytree min_child_weight subsample objective inter inter poly inter_order n.trees interaction.depth n.minobsinnode shrinkage mtry ntree kernel sigma C","code":""},{"path":[]},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"shallow-neural-networks","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Shallow Neural Networks","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Shallow Neural Network algorithm: size: Number units hidden layer. theory, can take positive integer greater equal 1. practice, limited hardware. Values high can lead memory issues, crashes overfitting. Values low can lead underfitting. decay: Weight decay parameter. can take values 0 1. Values high can lead underfitting. Values low can lead overfitting.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"deep-neural-networks-and-convolutional-neural-networks","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Deep Neural Networks and Convolutional Neural Networks","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Deep Convolutional Neural Networks algorithm: learning_rate: size gradient step taken optimization process. can take values 0 1. Values high can lead overshooting minimum diverging. Values low can lead slow convergence. n_epochs: Maximum number epochs train model. can take positive integer greater equal 1. Values high can lead overfitting. Values low can lead underfitting. batch_size: Size mini-batch used optimization process. can take positive integer greater equal 1 lower number samples. validation_patience: Number epochs improvement training stopped validation loop. can take positive integer greater equal 1. value equal higher number epochs, training stop. fitting_patience: Number epochs improvement training stopped final model fit. can take positive integer greater equal 1. value equal higher number epochs, training stop. hyperparameters like number hidden layers neurons layer set functions generate_arch_list, generate_dnn_architecture generate_cnn_architecture (see “Tuning example” section ).","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"extreme-gradient-boosting","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Extreme Gradient Boosting","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Extreme Gradient Boosting algorithm: nrounds: Max number boosting iterations. Can take positive integer greater equal 1. Values high can lead overfitting. Values low can lead underfitting. max_depth: maximum depth tree. Can take positive integer greater equal 1. eta: learning rate algorithm. can take values 0 1. Values high can lead overshooting minimum diverging. Values low can lead slow convergence. gamma: Minimum loss reduction required make partition leaf - node tree. range depends loss (objective) chosen. colsample_bytree: Subsample ratio columns constructing tree. can take values 1 number predictors (columns) min_child_weight: Minimum sum instance weight needed child. Can take non-negative value. subsample: Subsample ratio training instance. Can take values 0 1.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"generalized-additive-models","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Generalized Additive Models","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Generalized Additive Models algorithm: inter: Number knots x-axis. can take positive integer greater equal 1.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"generalized-linear-models","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Generalized Linear Models","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Generalized Additive Models algorithm: poly: Polynomials degree continuous variables (.e. used predictors argument). can take positive integer greater equal 2 inter_order: interaction order explanatory variables. can take positive integer greater equal 1.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"generalized-boosted-regression","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Generalized Boosted Regression","title":"Hyperparameter tuning in adm","text":"n.trees: total number trees fit. Can take positive integer greater equal 1. interaction.depth: maximum depth tree. Can take positive integer greater equal 1. Values high can lead overfitting. n.minobsinnode: minimum number observations terminal nodes trees. Can take positive integer greater equal 1. Values low high can lead overfitting. shrinkage: learning rate algorithm. Can take values 0 1. Values high can lead overshooting minimum diverging.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"random-forests","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Random Forests","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Random Forest algorithm: mtry: Number variables randomly sampled candidates split. can take values 1 number predictor variables dataset. ntree: Number trees grow. theory, can take positive integer greater equal 1. practice, limited hardware. Values high can lead memory issues crashes. Values low can lead underfitting.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"support-vector-machines","dir":"Articles","previous_headings":"What values the hyperparameters of my ADM can take?","what":"Support Vector Machines","title":"Hyperparameter tuning in adm","text":"Users can tune following hyperparameters Support Vector Machines algorithm: kernel: string defining kernel used algorithm. can take values described kernlab package. sigma: Either “automatic” (recommended) inverse kernel width Radial Basis kernel function “rbfdot” Laplacian kernel “laplacedot”. C: Cost constraints violation. can take positive integer greater equal 0. Values high can lead overfitting.","code":""},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"how-do-i-use-the-tuning-functions-with-the-grid-tuning-example","dir":"Articles","previous_headings":"","what":"How do I use the tuning functions with the grid? (Tuning example)","title":"Hyperparameter tuning in adm","text":"Tuning models adm straightforward. requires user provide grid hyperparameters values tested. construct hyperparameter grid, can use expand.grid list function straightforward way, passing one vector values hyperparameter. example Random Forest hyperparameters, approach can used algorithms: create data.frame 25 rows 2 columns, row combination hyperparameters. object can passed tune_abund functions grid argument (see ). grid created way number rows product number values hyperparameter, e.g., 3 values mtry 4 values ntree, grid 3 x 4 = 12 rows. row combination hyperparameters values. use grid, can pass tune_abund function grid argument:","code":"# Put each value of the hyperparameter in a vector, those in a list, and use expand.grid raf_grid <- expand.grid(   list(     mtry = c(1, 2, 3),     ntree = c(100, 200, 300, 400, 500)   ) ) head(raf_grid) #>   mtry ntree #> 1    1   100 #> 2    2   100 #> 3    3   100 #> 4    1   200 #> 5    2   200 #> 6    3   200 library(adm) #> Registered S3 method overwritten by 'bit': #>   method   from   #>   print.ri gamlss library(dplyr) #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union  # Load some data df <- adm::sppabund df <- df %>% filter(species == \"Species one\")  raf_adm <- adm::tune_abund_raf(    data = df,    response = \"ind_ha\",    metric = c(\"mae\"),   predictors = c(\"bio1\", \"bio12\", \"bio15\"),    partition = \".part1\",    grid = raf_grid # pass the grid here ) #> Using provided grid. #> Searching for optimal hyperparameters... #>   |                                                                              |                                                                      |   0%  |                                                                              |=====                                                                 |   7%  |                                                                              |=========                                                             |  13%  |                                                                              |==============                                                        |  20%  |                                                                              |===================                                                   |  27%  |                                                                              |=======================                                               |  33%  |                                                                              |============================                                          |  40%  |                                                                              |=================================                                     |  47%  |                                                                              |=====================================                                 |  53%  |                                                                              |==========================================                            |  60%  |                                                                              |===============================================                       |  67%  |                                                                              |===================================================                   |  73%  |                                                                              |========================================================              |  80%  |                                                                              |=============================================================         |  87%  |                                                                              |=================================================================     |  93%  |                                                                              |======================================================================| 100% #>  #> Fitting the best model... #> Formula used for model fitting: #> ind_ha ~ bio1 + bio12 + bio15 #> Replica number: 1/1 #> -- Partition number 1/3 #> -- Partition number 2/3 #> -- Partition number 3/3 #> The best model was achieved with:  #>  mtry = 1 and ntree = 500"},{"path":"https://xx.github.io/adm/articles/v00_hyperparameter_tuning.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Hyperparameter tuning in adm","text":"Tuning adm easy. Different algorithms distinct hyperparameters, user can choose ones tune values test. case, important aware hyperparameters limits ranges, well number combinations can generated, meaning. Consulting adm dependencies documentation good practice understand algorithms hyperparameters.","code":""},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"v01_modelling_workflow","text":"Abundance-based Distribution Models (ADM) promising approach construct spatially explicit correlative models species abundance. adm package allow users easily construct validate models, meeting researches specific needs. vignette, users learn support modeling workflow adm, data preparation model fitting prediction.","code":""},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"installation","dir":"Articles","previous_headings":"","what":"Installation","title":"v01_modelling_workflow","text":"","code":"require(adm) require(terra) require(dplyr)"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"preparaing-data","dir":"Articles","previous_headings":"","what":"Preparaing data","title":"v01_modelling_workflow","text":"tutorial, model abundace Cynophalla retusa (Griseb.) Cornejo & Iltis (Capparaceae), dry biome shrub native northeastern Argentina, Paraguay, Bolivia, central Brazil. predictor variables, use first 7 PC (cumulative viarance > 90%) PCA performed 35 climatic edaphic variables. can load needed data : Let’s explore data    cretusa_data 366 georeferenced points C. retusa. “ind_ha” column contains abundance measured individuals per hectare. “x” “y” decimals longitude latitude, respectively. “.part” folds spatial block partitioning used cross-validation. Now need extract environmental data predictors raster. , use adm_extract, columns x y coordinates important. Notice new dataframe one column environmental variable (layers raster). possible extract data specific layers using “variables” argument adm_extract. stabilize Deep Neural Networks training, transform response data using “zscore” method. creates new column called “ind_ha_zscore”, can used response variable.","code":"# Load species abundance data data(\"cretusa_data\")   # Load raster with environmental variables cretusa_predictors <- system.file(\"external/cretusa_predictors.tif\", package = \"adm\") cretusa_predictors <- terra::rast(cretusa_predictors) names(cretusa_predictors) #> [1] \"PC1\" \"PC2\" \"PC3\" \"PC4\" \"PC5\" \"PC6\" \"PC7\"  # Species training area sp_train_a <- system.file(\"external/cretusa_calib_area.gpkg\", package = \"adm\") sp_train_a <- terra::vect(sp_train_a) # Species data # ?cretusa_data cretusa_data # species dat #> # A tibble: 366 × 5 #>    species           ind_ha     x     y .part #>    <chr>              <int> <dbl> <dbl> <int> #>  1 Cynophalla retusa     10 -64.5 -22.7     1 #>  2 Cynophalla retusa     10 -64.1 -22.7     1 #>  3 Cynophalla retusa     20 -64.7 -23.1     2 #>  4 Cynophalla retusa      0 -62.6 -23.2     3 #>  5 Cynophalla retusa      0 -61.7 -24.5     3 #>  6 Cynophalla retusa      0 -61.6 -25.0     3 #>  7 Cynophalla retusa      0 -61.2 -24.5     3 #>  8 Cynophalla retusa      0 -64.9 -23.8     2 #>  9 Cynophalla retusa      0 -65.3 -24.4     3 #> 10 Cynophalla retusa      0 -64.7 -24.8     1 #> # ℹ 356 more rows # Environmental predictors names(cretusa_predictors) #> [1] \"PC1\" \"PC2\" \"PC3\" \"PC4\" \"PC5\" \"PC6\" \"PC7\" plot(cretusa_predictors) # Training area plot(sp_train_a) plot(cretusa_predictors[[1]]) plot(sp_train_a, add = TRUE) points(cretusa_data %>% dplyr::select(x, y), col = \"red\", pch = 20) species_data <- adm_extract(   data = cretusa_data, # georeferenced dataframe   x = \"x\", # spatial x coordinates   y = \"y\", # spatial y coordinates   env_layer = cretusa_predictors, # raster with environmental variables   variables = NULL, # return data for all layers   filter_na = TRUE )  species_data #> # A tibble: 366 × 12 #>    species    ind_ha     x     y .part    PC1    PC2    PC3   PC4    PC5     PC6 #>    <chr>       <int> <dbl> <dbl> <int>  <dbl>  <dbl>  <dbl> <dbl>  <dbl>   <dbl> #>  1 Cynophall…     10 -64.5 -22.7     1  1.50  -1.28   0.773 0.784 -0.249 -0.559  #>  2 Cynophall…     10 -64.1 -22.7     1  0.750 -1.22   1.36  0.359 -0.434 -0.742  #>  3 Cynophall…     20 -64.7 -23.1     2  1.21  -1.82   1.36  0.336 -0.862  0.0878 #>  4 Cynophall…      0 -62.6 -23.2     3 -1.71  -2.85  -1.34  0.762 -0.745 -1.02   #>  5 Cynophall…      0 -61.7 -24.5     3 -1.65  -2.17  -1.68  0.175 -0.839 -0.637  #>  6 Cynophall…      0 -61.6 -25.0     3 -1.29  -2.35  -1.94  0.614 -0.944 -0.805  #>  7 Cynophall…      0 -61.2 -24.5     3 -0.904 -2.73  -2.30  1.05  -0.604 -0.830  #>  8 Cynophall…      0 -64.9 -23.8     2  0.676 -1.57   0.401 1.54  -0.346 -0.417  #>  9 Cynophall…      0 -65.3 -24.4     3  0.460 -0.795  0.409 2.19  -0.172 -0.249  #> 10 Cynophall…      0 -64.7 -24.8     1  1.18   0.201  0.606 2.65  -0.356 -0.775  #> # ℹ 356 more rows #> # ℹ 1 more variable: PC7 <dbl> species_data <- adm_transform(   data = species_data,   variable = \"ind_ha\",   method = \"zscore\" )  species_data %>% dplyr::select(ind_ha, ind_ha_zscore) #> # A tibble: 366 × 2 #>    ind_ha ind_ha_zscore #>     <int>         <dbl> #>  1     10        0.0754 #>  2     10        0.0754 #>  3     20        0.802  #>  4      0       -0.651  #>  5      0       -0.651  #>  6      0       -0.651  #>  7      0       -0.651  #>  8      0       -0.651  #>  9      0       -0.651  #> 10      0       -0.651  #> # ℹ 356 more rows"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"tuning-models","dir":"Articles","previous_headings":"","what":"Tuning models","title":"v01_modelling_workflow","text":"set, good proceed ADM construction. tutorial, fine-tune, fit validate Deep Neural Network (DNN), Generalized Linear Models (GLM) Random Forest (RAF) models, using tune_abund_ family functions.","code":""},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"raf","dir":"Articles","previous_headings":"Tuning models","what":"RAF","title":"v01_modelling_workflow","text":"Starting RAF, first thing need determine values hyperparameters tested. done just part hyperparameters. example, set values . done creating grid guide values exploration, can easily constructed expand.grid base function: RAF, “mtry” determines number variables randomly sampled candidates split. setted values {1, 2, …, 6, 7}. “ntree” determine number decision trees grow. set values {100, 200, …, 900, 1000}. grid combines every possible pair values, totalizing 70 combinations (number rows “raf_grid”). Now can use grid tune_abund_raf tune validate RAF model: function returns list following elements: “model”: “randomForest” class object. “predictors”: tibble containing relevant informations model fitted. “performance”: tibble containing best models’ performance. “performance_part”: tibble performance partition. “predicted_part”: predictions partition. “optimal_combination”: set hyperparameters values considered best given metrics performance. “all_combinations”: performance every hyper-parameter combination.","code":"raf_grid <- expand.grid(   mtry = seq(from = 1, to = 7, by = 1),   ntree = seq(from = 100, to = 1000, by = 100) ) head(raf_grid) #>   mtry ntree #> 1    1   100 #> 2    2   100 #> 3    3   100 #> 4    4   100 #> 5    5   100 #> 6    6   100 nrow(raf_grid) # 70 combinations of these two hyper-paramenters #> [1] 70 mraf <- tune_abund_raf(   data = species_data,   response = \"ind_ha\",   predictors = c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\"),   partition = \".part\",   predict_part = TRUE, # predictions for every partition will be returned   grid = raf_grid,   metrics = c(\"corr_pear\", \"mae\"), # metrics to select the best model   n_cores = 4, # number of cores to be used in parallel processing   verbose = FALSE ) #> Using provided grid. #> Searching for optimal hyperparameters... #>   |                                                                              |                                                                      |   0%  |                                                                              |=                                                                     |   1%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |================                                                      |  23%  |                                                                              |=================                                                     |  24%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  40%  |                                                                              |=============================                                         |  41%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  61%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |==================================================                    |  71%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  77%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================| 100% #>  #> Fitting the best model... #> The best model was achieved with:  #>  mtry = 3 and ntree = 100 names(mraf) #> [1] \"model\"               \"predictors\"          \"performance\"         #> [4] \"performance_part\"    \"predicted_part\"      \"optimal_combination\" #> [7] \"all_combinations\" class(mraf$model) #> [1] \"randomForest.formula\" \"randomForest\" mraf$model #>  #> Call: #>  randomForest(formula = formula1, data = data, mtry = mtry, ntree = ntree,      importance = FALSE)  #>                Type of random forest: regression #>                      Number of trees: 100 #> No. of variables tried at each split: 3 #>  #>           Mean of squared residuals: 154.5952 #>                     % Var explained: 18.24 mraf$predictors #> # A tibble: 1 × 9 #>   model response c1    c2    c3    c4    c5    c6    c7    #>   <chr> <chr>    <chr> <chr> <chr> <chr> <chr> <chr> <chr> #> 1 raf   ind_ha   PC1   PC2   PC3   PC4   PC5   PC6   PC7 mraf$performance #> # A tibble: 1 × 13 #>   model mae_mean mae_sd corr_spear_mean corr_spear_sd corr_pear_mean #>   <chr>    <dbl>  <dbl>           <dbl>         <dbl>          <dbl> #> 1 raf       7.75   1.98           0.408         0.184          0.365 #> # ℹ 7 more variables: corr_pear_sd <dbl>, inter_mean <dbl>, inter_sd <dbl>, #> #   slope_mean <dbl>, slope_sd <dbl>, pdisp_mean <dbl>, pdisp_sd <dbl> mraf$performance_part #> # A tibble: 3 × 9 #>   replica partition model   mae corr_spear corr_pear  inter slope pdisp #>   <chr>   <chr>     <chr> <dbl>      <dbl>     <dbl>  <dbl> <dbl> <dbl> #> 1 1       1         raf    9.92      0.618     0.496 -0.435 1.23  0.404 #> 2 1       2         raf    7.29      0.334     0.331  2.42  0.502 0.659 #> 3 1       3         raf    6.04      0.272     0.268  2.07  0.574 0.467 mraf$predicted_part %>% head() #> # A tibble: 6 × 4 #>   replica partition observed predicted #>   <chr>   <chr>        <int>     <dbl> #> 1 1       1               10      1.14 #> 2 1       1               10      3.23 #> 3 1       1                0      0.75 #> 4 1       1               10      7.03 #> 5 1       1               10      7.58 #> 6 1       1               10      6.51 mraf$optimal_combination %>% dplyr::glimpse() #> Rows: 1 #> Columns: 16 #> $ comb_id         <chr> \"comb_3\" #> $ mtry            <dbl> 3 #> $ ntree           <dbl> 100 #> $ model           <chr> \"raf\" #> $ mae_mean        <dbl> 7.752958 #> $ mae_sd          <dbl> 1.978733 #> $ corr_spear_mean <dbl> 0.4077742 #> $ corr_spear_sd   <dbl> 0.1843192 #> $ corr_pear_mean  <dbl> 0.3649869 #> $ corr_pear_sd    <dbl> 0.1175491 #> $ inter_mean      <dbl> 1.352334 #> $ inter_sd        <dbl> 1.557785 #> $ slope_mean      <dbl> 0.7677054 #> $ slope_sd        <dbl> 0.3987553 #> $ pdisp_mean      <dbl> 0.5099857 #> $ pdisp_sd        <dbl> 0.132572 mraf$all_combinations %>% head() #> # A tibble: 6 × 16 #>   comb_id  mtry ntree model mae_mean mae_sd corr_spear_mean corr_spear_sd #>   <chr>   <dbl> <dbl> <chr>    <dbl>  <dbl>           <dbl>         <dbl> #> 1 comb_1      1   100 raf       7.81   2.02           0.403         0.203 #> 2 comb_2      2   100 raf       7.75   2.03           0.407         0.201 #> 3 comb_3      3   100 raf       7.75   1.98           0.408         0.184 #> 4 comb_4      4   100 raf       7.91   2.07           0.384         0.182 #> 5 comb_5      5   100 raf       7.75   2.03           0.415         0.176 #> 6 comb_6      6   100 raf       7.83   1.99           0.396         0.181 #> # ℹ 8 more variables: corr_pear_mean <dbl>, corr_pear_sd <dbl>, #> #   inter_mean <dbl>, inter_sd <dbl>, slope_mean <dbl>, slope_sd <dbl>, #> #   pdisp_mean <dbl>, pdisp_sd <dbl>"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"glm","dir":"Articles","previous_headings":"Tuning models","what":"GLM","title":"v01_modelling_workflow","text":"tune GLM perform basically steps tune RAF, paying attention GLM singularities. First, need construct grid, just like . However, GLM needs “distribution” hyper-parameter specifies probability distribution family used. Choosing distribution family needs attention must done wisely, adm provides help via family_selector function. function compares response variable range gamlss compatible families: function returns tibble suitable families information. column “family_call” can directly used grid. interested exploring attributes families GLM GAM, can use families_bank database. details family distributions see ?gamlss.dist::gamlss.family. example, selected suitable distributions use. Now, can construct grid: GLM, “poly” refers polynomials used model formula, “inter_order” refers interaction order variables. Tuning GLM grid: output list basically elements tune_abund_raf, don’t need go . difference “model”, now “gamlss” class object.","code":"suitable_families <- family_selector(   data = species_data,   response = \"ind_ha\" ) #> Response variable is discrete. Both continuous and discrete families will be tested. #> Selected 61 suitable families for the data. fm <- system.file(\"external/families_bank.txt\", package = \"adm\") %>%   utils::read.delim(., header = TRUE, quote = \"\\t\") %>%   dplyr::as_tibble() fm #> # A tibble: 87 × 9 #>    family_name             family_call range no_parameters discrete accepts_zero #>    <chr>                   <chr>       <chr>         <int>    <int>        <int> #>  1 Beta                    BE          (0,1)             2        0            0 #>  2 Beta one inflated       BEOI        (0,1]             3        0            0 #>  3 Box-Cox Cole and Green  BCCG        (0, …             3        0            0 #>  4 Box-Cox Power Exponent… BCPE        (0, …             4        0            0 #>  5 Box-Cox-t               BCT         (0, …             4        0            0 #>  6 Exponential             EXP         (0, …             1        0            0 #>  7 Gamma                   GA          (0, …             2        0            0 #>  8 Generalized Beta type 1 GB1         (0,1)             4        0            0 #>  9 Generalized Beta type 2 GB2         (0, …             4        0            0 #> 10 Generalized Gamma       GG          (0, …             3        0            0 #> # ℹ 77 more rows #> # ℹ 3 more variables: one_restricted <int>, accepts_one <int>, #> #   accepts_negatives <int> glm_grid <- list(   distribution = c(     \"NO\", \"NOF\", \"RG\", \"TF\", \"ZAIG\", \"LQNO\", \"DEL\",     \"PIG\", \"WARING\", \"YULE\", \"ZALG\", \"ZIP\", \"BNB\",     \"DBURR12\", \"ZIBNB\", \"LO\", \"PO\"   ),   poly = c(1, 2, 3),   inter_order = c(0, 1, 2) ) %>%   expand.grid() # Note that in `distribution` argument it is necessary use the acronyms of `family_call` column mglm <- tune_abund_glm(   data = species_data,   response = \"ind_ha\",   predictors = c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\"),   predictors_f = NULL,   partition = \".part\",   predict_part = TRUE,   grid = glm_grid,   metrics = c(\"corr_pear\", \"mae\"),   n_cores = 4,   verbose = FALSE ) #> Using provided grid. #> Searching for optimal hyperparameters... #>   |                                                                              |                                                                      |   0%  |                                                                              |                                                                      |   1%  |                                                                              |=                                                                     |   1%  |                                                                              |=                                                                     |   2%  |                                                                              |==                                                                    |   3%  |                                                                              |===                                                                   |   4%  |                                                                              |===                                                                   |   5%  |                                                                              |====                                                                  |   5%  |                                                                              |====                                                                  |   6%  |                                                                              |=====                                                                 |   7%  |                                                                              |=====                                                                 |   8%  |                                                                              |======                                                                |   8%  |                                                                              |======                                                                |   9%  |                                                                              |=======                                                               |  10%  |                                                                              |========                                                              |  11%  |                                                                              |========                                                              |  12%  |                                                                              |=========                                                             |  12%  |                                                                              |=========                                                             |  13%  |                                                                              |==========                                                            |  14%  |                                                                              |===========                                                           |  15%  |                                                                              |===========                                                           |  16%  |                                                                              |============                                                          |  17%  |                                                                              |============                                                          |  18%  |                                                                              |=============                                                         |  18%  |                                                                              |=============                                                         |  19%  |                                                                              |==============                                                        |  20%  |                                                                              |===============                                                       |  21%  |                                                                              |===============                                                       |  22%  |                                                                              |================                                                      |  22%  |                                                                              |================                                                      |  23%  |                                                                              |================                                                      |  24%  |                                                                              |=================                                                     |  24%  |                                                                              |=================                                                     |  25%  |                                                                              |==================                                                    |  25%  |                                                                              |==================                                                    |  26%  |                                                                              |===================                                                   |  27%  |                                                                              |====================                                                  |  28%  |                                                                              |====================                                                  |  29%  |                                                                              |=====================                                                 |  29%  |                                                                              |=====================                                                 |  30%  |                                                                              |======================                                                |  31%  |                                                                              |======================                                                |  32%  |                                                                              |=======================                                               |  33%  |                                                                              |========================                                              |  34%  |                                                                              |========================                                              |  35%  |                                                                              |=========================                                             |  35%  |                                                                              |=========================                                             |  36%  |                                                                              |==========================                                            |  37%  |                                                                              |===========================                                           |  38%  |                                                                              |===========================                                           |  39%  |                                                                              |============================                                          |  40%  |                                                                              |============================                                          |  41%  |                                                                              |=============================                                         |  41%  |                                                                              |=============================                                         |  42%  |                                                                              |==============================                                        |  42%  |                                                                              |==============================                                        |  43%  |                                                                              |===============================                                       |  44%  |                                                                              |================================                                      |  45%  |                                                                              |================================                                      |  46%  |                                                                              |=================================                                     |  47%  |                                                                              |=================================                                     |  48%  |                                                                              |==================================                                    |  48%  |                                                                              |==================================                                    |  49%  |                                                                              |===================================                                   |  50%  |                                                                              |====================================                                  |  51%  |                                                                              |====================================                                  |  52%  |                                                                              |=====================================                                 |  52%  |                                                                              |=====================================                                 |  53%  |                                                                              |======================================                                |  54%  |                                                                              |======================================                                |  55%  |                                                                              |=======================================                               |  56%  |                                                                              |========================================                              |  57%  |                                                                              |========================================                              |  58%  |                                                                              |=========================================                             |  58%  |                                                                              |=========================================                             |  59%  |                                                                              |==========================================                            |  59%  |                                                                              |==========================================                            |  60%  |                                                                              |===========================================                           |  61%  |                                                                              |===========================================                           |  62%  |                                                                              |============================================                          |  63%  |                                                                              |=============================================                         |  64%  |                                                                              |=============================================                         |  65%  |                                                                              |==============================================                        |  65%  |                                                                              |==============================================                        |  66%  |                                                                              |===============================================                       |  67%  |                                                                              |================================================                      |  68%  |                                                                              |================================================                      |  69%  |                                                                              |=================================================                     |  70%  |                                                                              |=================================================                     |  71%  |                                                                              |==================================================                    |  71%  |                                                                              |==================================================                    |  72%  |                                                                              |===================================================                   |  73%  |                                                                              |====================================================                  |  74%  |                                                                              |====================================================                  |  75%  |                                                                              |=====================================================                 |  75%  |                                                                              |=====================================================                 |  76%  |                                                                              |======================================================                |  76%  |                                                                              |======================================================                |  77%  |                                                                              |======================================================                |  78%  |                                                                              |=======================================================               |  78%  |                                                                              |=======================================================               |  79%  |                                                                              |========================================================              |  80%  |                                                                              |=========================================================             |  81%  |                                                                              |=========================================================             |  82%  |                                                                              |==========================================================            |  82%  |                                                                              |==========================================================            |  83%  |                                                                              |===========================================================           |  84%  |                                                                              |===========================================================           |  85%  |                                                                              |============================================================          |  86%  |                                                                              |=============================================================         |  87%  |                                                                              |=============================================================         |  88%  |                                                                              |==============================================================        |  88%  |                                                                              |==============================================================        |  89%  |                                                                              |===============================================================       |  90%  |                                                                              |================================================================      |  91%  |                                                                              |================================================================      |  92%  |                                                                              |=================================================================     |  92%  |                                                                              |=================================================================     |  93%  |                                                                              |==================================================================    |  94%  |                                                                              |==================================================================    |  95%  |                                                                              |===================================================================   |  95%  |                                                                              |===================================================================   |  96%  |                                                                              |====================================================================  |  97%  |                                                                              |===================================================================== |  98%  |                                                                              |===================================================================== |  99%  |                                                                              |======================================================================|  99%  |                                                                              |======================================================================| 100% #>  #> Fitting the best model... #> The best model was achieved with: #>  distribution = PO #>  poly = 2 #>  inter_order = 0 class(mglm$model) #> [1] \"gamlss\" \"gam\"    \"glm\"    \"lm\" mglm$model #>  #> Family:  c(\"PO\", \"Poisson\")  #> Fitting method: RS()  #>  #> Call:  gamlss::gamlss(formula = formula1, sigma.formula = sigma_formula,   #>     nu.formula = nu_formula, tau.formula = tau_formula,   #>     family = family, data = data, control = control_gamlss,      trace = FALSE)  #>  #>  #> Mu Coefficients: #> (Intercept)          PC1          PC2          PC3          PC4          PC5   #>   -0.539373     0.426482    -1.486044     0.500924     0.074328     0.447586   #>         PC6          PC7     I(PC1^2)     I(PC2^2)     I(PC3^2)     I(PC4^2)   #>   -1.497579     0.290151    -0.372255     0.004844     0.072316    -0.382721   #>    I(PC5^2)     I(PC6^2)     I(PC7^2)   #>    0.684274    -0.697127    -1.088711   #>  #>  Degrees of Freedom for the fit: 15 Residual Deg. of Freedom   351  #> Global Deviance:     4604.94  #>             AIC:     4634.94  #>             SBC:     4693.48"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"dnn","dir":"Articles","previous_headings":"Tuning models","what":"DNN","title":"v01_modelling_workflow","text":"DNN, addition grid, function tune_abund_dnn also needs list architectures test, single one. use generate_arch_list function purpose, . However, things get complicated. DNN architecture aspects number size layers, batch normalization, dropout customizable adm, many architectures generated , possible combinations parameters, resulting large lists. filter list, users can use select_arch_list reduce , sampling list using number parameters net complexity measurement. highly recommended. Let’s create select architectures: However, sake brevity tutorial, manually reduce even architectures list just : Now can construct grid hyper-parameters combinations. Note hyper-parameters values tested architecture. Therefore user needs careful grid architecture list sizes. Now can use architectures generated grid created within tune_abund_dnn function: , output similar , standardize tune_abund_ functions. Now, “model” element “luz_module_fitted” torch luz packages.","code":"archs <- adm::generate_arch_list(   type = \"dnn\",   number_of_features = 7, # input/predictor variables   number_of_outputs = 1, # output/response variable   n_layers = c(2, 3, 4), # possible number of layers   n_neurons = c(7, 14, 21), # possible number of neurons on each layer   batch_norm = TRUE, # batch normalization between layers   dropout = 0 # without training dropout )  number_before <- archs$arch_list %>% length() # 117  archs <- adm::select_arch_list(   arch_list = archs,   type = \"dnn\",   method = \"percentile\", # sample by number of parameters percentiles   n_samples = 2, # at least two with each number of layers   min_max = TRUE # keep the more simple and the more complex networks )  number_after <- archs$arch_list %>% length() # 52  message(\"From \", number_before, \" to \", number_after, \" architectures.\") #> From 117 to 52 architectures. archs$arch_list <- archs$arch_list[seq(from = 1, to = 52, by = 5)]  length(archs$arch_list) #> [1] 11 dnn_grid <- expand.grid(   batch_size = c(64),   validation_patience = c(5),   fitting_patience = c(5),   learning_rate = c(0.005, 0.001, 0.0005),   n_epochs = 200 ) head(dnn_grid) #>   batch_size validation_patience fitting_patience learning_rate n_epochs #> 1         64                   5                5         5e-03      200 #> 2         64                   5                5         1e-03      200 #> 3         64                   5                5         5e-04      200 nrow(dnn_grid) #> [1] 3 mdnn <- tune_abund_dnn(   data = species_data,   response = \"ind_ha_zscore\", # using the transformed response   predictors = c(\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\"),   predictors_f = NULL,   partition = \".part\",   predict_part = TRUE,   grid = dnn_grid,   architectures = archs,   metrics = c(\"corr_pear\", \"mae\"),   n_cores = 10,   verbose = FALSE ) #> Using provided architectures. #> Using provided grid. #> Testing 33 combinations. #> Searching for optimal hyperparameters... #>   |                                                                              |                                                                      |   0%  |                                                                              |==                                                                    |   3%  |                                                                              |====                                                                  |   6%  |                                                                              |======                                                                |   9%  |                                                                              |========                                                              |  12%  |                                                                              |===========                                                           |  15%  |                                                                              |=============                                                         |  18%  |                                                                              |===============                                                       |  21%  |                                                                              |=================                                                     |  24%  |                                                                              |===================                                                   |  27%  |                                                                              |=====================                                                 |  30%  |                                                                              |=======================                                               |  33%  |                                                                              |=========================                                             |  36%  |                                                                              |============================                                          |  39%  |                                                                              |==============================                                        |  42%  |                                                                              |================================                                      |  45%  |                                                                              |==================================                                    |  48%  |                                                                              |====================================                                  |  52%  |                                                                              |======================================                                |  55%  |                                                                              |========================================                              |  58%  |                                                                              |==========================================                            |  61%  |                                                                              |=============================================                         |  64%  |                                                                              |===============================================                       |  67%  |                                                                              |=================================================                     |  70%  |                                                                              |===================================================                   |  73%  |                                                                              |=====================================================                 |  76%  |                                                                              |=======================================================               |  79%  |                                                                              |=========================================================             |  82%  |                                                                              |===========================================================           |  85%  |                                                                              |==============================================================        |  88%  |                                                                              |================================================================      |  91%  |                                                                              |==================================================================    |  94%  |                                                                              |====================================================================  |  97%  |                                                                              |======================================================================| 100% #>  #> Fitting the best model... #> The best model was achieved with:  #>  learning_rate = 0.005 #>  n_epochs = 200 #>  patience = 5 and 5 #>  batch_size = 64 #>  arch = 4 layers with 21->7->21->14 neurons class(mdnn$model) #> [1] \"luz_module_fitted\" mdnn$model #> A `luz_module_fitted` #> ── Time ──────────────────────────────────────────────────────────────────────── #> • Total time: 4.9s #> • Avg time per training epoch: 193ms #>  #> ── Results ───────────────────────────────────────────────────────────────────── #> Metrics observed in the last epoch. #>  #> ℹ Training: #> loss: 0.4708 #>  #> ── Model ─────────────────────────────────────────────────────────────────────── #> An `nn_module` containing 939 parameters. #>  #> ── Modules ───────────────────────────────────────────────────────────────────── #> • linear1: <nn_linear> #168 parameters #> • linear2: <nn_linear> #154 parameters #> • linear3: <nn_linear> #168 parameters #> • linear4: <nn_linear> #308 parameters #> • output: <nn_linear> #15 parameters #> • bn1: <nn_batch_norm1d> #42 parameters #> • bn2: <nn_batch_norm1d> #14 parameters #> • bn3: <nn_batch_norm1d> #42 parameters #> • bn4: <nn_batch_norm1d> #28 parameters"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"summarizing-results","dir":"Articles","previous_headings":"Tuning models","what":"Summarizing results","title":"v01_modelling_workflow","text":"adm possible quick summarize several models evalutions one dataframe, using adm_summarize function:","code":"adm_summarize(list(mdnn, mraf, mglm)) #> # A tibble: 3 × 14 #>   model_ID model mae_mean mae_sd corr_spear_mean corr_spear_sd corr_pear_mean #>      <int> <chr>    <dbl>  <dbl>           <dbl>         <dbl>          <dbl> #> 1        1 dnn      0.544  0.174           0.416         0.129          0.393 #> 2        2 raf      7.75   1.98            0.408         0.184          0.365 #> 3        3 glm      7.59   2.17            0.492         0.162          0.427 #> # ℹ 7 more variables: corr_pear_sd <dbl>, inter_mean <dbl>, inter_sd <dbl>, #> #   slope_mean <dbl>, slope_sd <dbl>, pdisp_mean <dbl>, pdisp_sd <dbl>"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"predicting-models","dir":"Articles","previous_headings":"","what":"Predicting models","title":"v01_modelling_workflow","text":"fitted models, can make spatial predictions . process straightforward adm_predict function. can make predictions one model . illustration purposes, make predictions calibration area delimited 100 km buffered minimum convex polygon around species presence points, process optional. predict DNN, need pay attention detail. trained DNN tranformed response, want predict original scale, need use “invert_transform” argument adm_predict: Note: transformation terms varies among tranformation methods. learn , visit adm_transform documentation. Let’s visualize predictions:","code":"sp_train_a <- system.file(\"external/cretusa_calib_area.gpkg\", package = \"adm\") sp_train_a <- terra::vect(sp_train_a)  preds <- adm::adm_predict(   models = list(mraf, mglm),   pred = cretusa_predictors,   predict_area = sp_train_a,   training_data = species_data,   transform_negative = TRUE # negative predictions will be considered 0 ) #> Predicting list of individual models pred_dnn <- adm_predict(   models = mdnn,   pred = cretusa_predictors,   predict_area = sp_train_a,   training_data = species_data,   transform_negative = TRUE,   invert_transform = c(     method = \"zscore\",     a = mean(species_data$ind_ha),     b = sd(species_data$ind_ha)   ) ) #> Predicting individual models par(mfrow = c(1, 3)) plot(pred_dnn$dnn, main = \"DNN\") plot(preds$raf, main = \"RAF\") plot(preds$glm, main = \"GLM\") par(mfrow = c(1, 1))"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"exploring-models","dir":"Articles","previous_headings":"","what":"Exploring models","title":"v01_modelling_workflow","text":"Finally, can explore variable pair variables affect predicted values. adm features univariate bivariate Partial Dependence Plots, PDP, BPDP, respectively. create PDP use p_abund_pdp. PDP illustrates marginal response one predictor. can provide relevant information residuals model extrapolation:    BPDP similar PDP, instead one, illustrates marginal response pair variables. example use first seventh PC. Note p_abund_pdp p_abund_bpdp, subset predictors can used “predictors” argument. “predictors” argument NULL, functions plot variables variables pair combinations, respectively.","code":"# PDP for DNN model pdp_dnn <- p_abund_pdp(   model = mdnn, # the output of tune_abund_ or fit_abund_   predictors = c(\"PC1\"),   resolution = 100,   resid = TRUE, # plot residuals   training_data = species_data,   invert_transform = c(     method = \"zscore\", # same as before     a = mean(species_data$ind_ha),     b = sd(species_data$ind_ha)   ),   response_name = \"ind/ha\", # this argument is for aesthetic only, and determines the name of y axis   projection_data = cretusa_predictors, # to visualize extrapolation   rug = TRUE, # rug plot of the predictor   colorl = c(\"#462777\", \"#6DCC57\"), # projection and training values, respectively   colorp = \"black\", # residuals colors   alpha = 0.2,   theme = ggplot2::theme_classic() # a ggplot2 theme )  # PDP for GLM model pdp_glm <- p_abund_pdp(   model = mglm,   predictors = c(\"PC1\"),   resolution = 100,   resid = TRUE,   training_data = species_data,   response_name = \"ind/ha\",   projection_data = cretusa_predictors,   rug = TRUE,   colorl = c(\"#462777\", \"#6DCC57\"),   colorp = \"black\",   alpha = 0.2,   theme = ggplot2::theme_classic() )  # PDP for RAF model pdp_raf <- p_abund_pdp(   model = mraf,   predictors = c(\"PC1\"),   resolution = 100,   resid = TRUE,   training_data = species_data,   response_name = \"ind/ha\",   projection_data = cretusa_predictors,   rug = TRUE,   colorl = c(\"#462777\", \"#6DCC57\"),   colorp = \"black\",   alpha = 0.2,   theme = ggplot2::theme_classic() ) pdp_dnn pdp_raf pdp_glm # BPDP for DNN bpdp_dnn <- p_abund_bpdp(   model = mdnn,   predictors = c(\"PC1\", \"PC7\"), # a pair of predictors   resolution = 100,   training_data = species_data,   projection_data = cretusa_predictors,   training_boundaries = \"convexh\", # the shape in which the training boundaries are drawn. Outside of it, its extrapolations   invert_transform = c(     method = \"zscore\", # same as before     a = mean(species_data$ind_ha),     b = sd(species_data$ind_ha)   ),   response_name = \"ind/ha\",   color_gradient = c(     \"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\",     \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\"   ), # gradient for response variable   color_training_boundaries = \"white\",   theme = ggplot2::theme_classic() )  # BPDP for GLM bpdp_glm <- p_abund_bpdp(   model = mglm,   predictors = c(\"PC1\", \"PC7\"),   resolution = 100,   training_data = species_data,   projection_data = cretusa_predictors,   training_boundaries = \"convexh\",   response_name = \"ind/ha\",   color_gradient = c(     \"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\",     \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\"   ),   color_training_boundaries = \"white\",   theme = ggplot2::theme_classic() )  # BPDP for RAF bpdp_raf <- p_abund_bpdp(   model = mraf,   predictors = c(\"PC1\", \"PC7\"),   resolution = 100,   training_data = species_data,   projection_data = cretusa_predictors,   training_boundaries = \"convexh\",   response_name = \"ind/ha\",   color_gradient = c(     \"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\",     \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\"   ),   color_training_boundaries = \"white\",   theme = ggplot2::theme_classic() ) bpdp_dnn bpdp_raf bpdp_glm"},{"path":"https://xx.github.io/adm/articles/v01_modelling_workflow.html","id":"conclusions","dir":"Articles","previous_headings":"","what":"Conclusions","title":"v01_modelling_workflow","text":"vignette, explored adm tools support complete ADM workflows. package features functions model tuning, fitting, validation model prediction exploration.","code":""},{"path":"https://xx.github.io/adm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"x x. Author. x x. Author, maintainer.","code":""},{"path":"https://xx.github.io/adm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"x x, x x (2025). adm: Tools Constructing, Predicting, Post-Processing Abundance-based Distribution Models. R package version 0.0.2.","code":"@Manual{,   title = {adm: Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models},   author = {x x and x x},   year = {2025},   note = {R package version 0.0.2}, }"},{"path":[]},{"path":"https://xx.github.io/adm/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"package aims support construction Abundance-based distribution models, including data preparation, model fitting, prediction, model exploration. package offers several modeling approaches (.e., algorithms), can fine-tuned customized users. Models can predicted geographic space explored terms performance response curves. modeling workflows adm constructed based combination distinct functions simple outputs, adm can easily integrated packages.","code":""},{"path":"https://xx.github.io/adm/index.html","id":"structure-of-adm","dir":"","previous_headings":"","what":"Structure of adm","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"adm functions grouped three categories: modeling, post-modeling, miscellaneus tools","code":""},{"path":"https://xx.github.io/adm/index.html","id":"i-modeling","dir":"","previous_headings":"Structure of adm","what":"i) modeling","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Functions tune, fit validate models nine different algorithms, suite possible model-specific hyper-parametera Fit validate models without hyper-parameters tuning fit_abund_cnn(): Fit validate Convolutional Neural Network Model fit_abund_dnn(): Fit validate Deep Neural Network model fit_abund_gam(): Fit validate Generalized Additive Models fit_abund_gbm(): Fit validate Generalized Boosted Regression models fit_abund_glm(): Fit validate Generalized Linear Models fit_abund_net(): Fit validate Artificial Neural Network models fit_abund_raf(): Fit validate Random Forests models fit_abund_svm(): Fit validate Support Vector Machine models fit_abund_xgb(): Fit validate Extreme Gradient Boosting models Fit validate models hyper-parameters tuning tune_abund_cnn(): Fit validate Convolutional Neural Network exploration hyper-parameters optimize performance tune_abund_dnn(): Fit validate Deep Neural Network model exploration hyper-parameters optimize performance tune_abund_gam(): Fit validate Generalized Additive Models exploration hyper-parameters optimize performance tune_abund_gbm(): Fit validate Generalized Boosted Regression models exploration hyper-parameters optimize performance tune_abund_glm(): Fit validate Generalized Linear Models exploration hyper-parameters optimize performance tune_abund_net(): Fit validate Shallow Neural Networks models exploration hyper-parameters optimize performance tune_abund_raf(): Fit validate Random Forest models exploration hyper-parameters optimize performance tune_abund_svm(): Fit validate Support Vector Machine models exploration hyper-parameters optimize performance tune_abund_xgb(): Fit validate Extreme Gradient Boosting models exploration hyper-parameters optimize performance Modeling evalution adm_eval(): Calculate different model performance metrics","code":""},{"path":"https://xx.github.io/adm/index.html","id":"ii-post-modeling","dir":"","previous_headings":"Structure of adm","what":"ii) post-modeling","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Functions predict abundance across space construct partial dependence plots explore relationships abundance environmental predictors adm_predict(): Spatial predictions individual ensemble models p_abund_bpdp(): Bivariate partial dependence plots abundance-based distribution models p_abund_pdp(): Partial dependent plots abundance-based distribution models data_abund_bpdp(): Calculate data construct bivariate partial dependence plots data_abund_pdp(): Calculate data construct partial dependence plots","code":""},{"path":"https://xx.github.io/adm/index.html","id":"iii-miscellaneous-tools","dir":"","previous_headings":"Structure of adm","what":"iii) miscellaneous tools","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Extra functions support modeling workflow, including data handling, transformations, hyper-parameters selection. adm_extract(): Extract values spatial raster based x y coordinates adm_summarize(): Merge model performance tables adm_transform(): Performs data transformation variable based specified method. balance_dataset(): Balance database given absence-presence ratio cnn_make_samples(): Creates sample data Convolutional Neural Network croppin_hood(): Crop rasters around point (Convolutional Neural Networks) family_selector(): Select probability distributions GAM GLM generate_arch_list(): Generate architecture list Deep Neural Network Convolutional Neural Network generate_cnn_architecture(): Generate architectures Convolutional Neural Network generate_dnn_architecture(): Generate architectures Deep Neural Network model_selection(): Best hyper-parameters selection res_calculate(): Calculate output resolution layer select_arch_list(): Select architectures Convolutional Neural Network Deep Neural Network","code":""},{"path":"https://xx.github.io/adm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"can install development version adm github","code":"# For Windows and Mac OS operating systems remotes::install_github(\"x/adm\")"},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate different model performance metrics — adm_eval","title":"Calculate different model performance metrics — adm_eval","text":"function, supplied observed predicted values, calculates  accuracy, discrimination, precision two returns values tibble table. accuracy evaluated mean absolute error. discrimination calculated using Spearman correlation, Pearson correlation, intercept slope linear regression observed predicted values. precision obtained standard deviations predicted observed values.","code":""},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate different model performance metrics — adm_eval","text":"","code":"adm_eval(obs, pred)"},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate different model performance metrics — adm_eval","text":"obs numeric. Observed abundance pred numeric. Predicted abundance","code":""},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate different model performance metrics — adm_eval","text":"tibble next columns: corr_spear, corr_pear, mae, inter, slope, pdisp(see details)","code":""},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate different model performance metrics — adm_eval","text":"function calculate metric related accuracy, discrimination, precision model: Accuracy: mean absolute error (mae) Discrimination: Spearman's rank correlation (corr_spear) Discrimination: Pearson's correlation (corr_pear) Discrimination: regression intercept observed predicted values (inter) Discrimination: regression slope observed predicted values (slope) Precision: ratio predicted observed standard deviation (pdisp) details see Waldock et al. (2022)","code":""},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate different model performance metrics — adm_eval","text":"Waldock, C., Stuart-Smith, R.D., Albouy, C., Cheung, W.W.L., Edgar, G.J., Mouillot, D., Tjiputra, J., Pellissier, L., 2022. quantitative review abundance-based species distribution models. Ecography https://doi.org/10.1111/ecog.05694","code":""},{"path":"https://xx.github.io/adm/reference/adm_eval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate different model performance metrics — adm_eval","text":"","code":"if (FALSE) { pred_a <- c(   3, 2, 0, 0, 2, 5, 1, 3, 1, 2, 1, 1, 2, 5, 4,   1, 2, 5, 3, 3, 4, 3, 2, 0, 2, 1, 2, 2, 1, 4,   4, 2, 2, 1, 6, 1, 1, 3, 5, 0, 1, 1, 0, 1, 2 ) obs_a <- c(   3, 1, 1, 3, 2, 3, 0, 3, 5, 3, 4, 2, 0, 5, 2,   1, 2, 2, 3, 6, 3, 2, 4, 2, 1, 2, 3, 5, 0, 3,   3, 2, 1, 2, 3, 2, 2, 1, 2, 3, 3, 1, 2, 1, 4 )  adm_eval(obs = obs_a, pred = pred_a) }"},{"path":"https://xx.github.io/adm/reference/adm_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from a spatial raster based on x and y coordinates — adm_extract","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"function extracts environmental data given x y coordinates","code":""},{"path":"https://xx.github.io/adm/reference/adm_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"","code":"adm_extract(data, x, y, env_layer, variables = NULL, filter_na = TRUE)"},{"path":"https://xx.github.io/adm/reference/adm_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"data data.frame tibble. Database species abundance x y coordinates x character. Column name spatial x coordinates y character. Column name spatial y coordinates env_layer SpatRaster. Raster environmental variables. variables character. Vector variable names predictor (environmental) variables Usage variables = c(\"elevation\", \"sand\", \"cfvo\"). variable specified, function return data layers. Default NULL filter_na logical. filter_na = TRUE (default), rows NA values environmental variables removed returned tibble.","code":""},{"path":"https://xx.github.io/adm/reference/adm_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"tibble returns original data base additional columns extracted environmental variables x y location SpatRaster object used 'env_layer'","code":""},{"path":"https://xx.github.io/adm/reference/adm_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"","code":"if (FALSE) { require(terra)  # Datasbase with species abundance and x and y coordinates data(\"sppabund\")  # Raster data with environmental variables envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Extract data for a single species some_sp <- sppabund %>%   filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  # Extract environmental data from envar raster for all locations in spp ex_spp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar,     variables = NULL,     filter_na = FALSE   )  # Extract environmental for two variables and remove rows with NAs ex_spp2 <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar,     variables = c(\"bio1\", \"elevation\"),     filter_na = TRUE   )  ex_spp ex_spp2 }"},{"path":"https://xx.github.io/adm/reference/adm_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial predictions from individual and ensemble models — adm_predict","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"function allows geographical prediction one models constructed fit_ tune_ function set, models fitted esm_ function set (.e., ensemble small models approach), models constructed fit_ensemble function. can return continuous continuous binary predictions one thresholds","code":""},{"path":"https://xx.github.io/adm/reference/adm_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"","code":"adm_predict(   models,   pred,   training_data = NULL,   nchunk = 1,   predict_area = NULL,   invert_transform = NULL,   transform_negative = FALSE,   sample_size = NULL )"},{"path":"https://xx.github.io/adm/reference/adm_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"models list one models fitted fit_ tune_ functions. case use models fitted fit_ensemble esm_ family function one model used. Usage models = mglm models = list(mglm, mraf, mgbm) pred SpatRaster. Raster layer predictor variables. Names layers must exactly match used model fitting. training_data data.frame tibble. Data used fit models. necessary predict GAM GLM models. Default NULL nchunk integer. Number chunks split data used predict models (.e., SpatRaster used pred argument). Predicting models chunks helps reduce memory requirements cases models predicted large scales high resolution. Default = 1 predict_area SpatVector, SpatialPolygon, SpatialPolygonDataFrame. Spatial polygon used restring prediction given region. Default = NULL invert_transform named vector. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Usage = c(method = \"anymethod\", = \"transformation term \", b = \"transformation term b\"). Default NULL transform_negative logical. TRUE, negative values prediction set zero. default FALSE. sample_size numeric. vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11)","code":""},{"path":"https://xx.github.io/adm/reference/adm_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"list SpatRaster continuous /binary predictions","code":""},{"path":"https://xx.github.io/adm/reference/adm_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"","code":"if (FALSE) { require(dplyr) require(terra)  data(\"sppabund\") envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Extract data some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  some_sp  some_sp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar   )  # Partition some_sp <- flexsdm::part_random(   data = some_sp,   pr_ab = \"ind_ha\",   method = c(method = \"rep_kfold\", folds = 3, replicates = 3) )   ## %######################################################%## #                                                          # ####          Create different type of models           #### #                                                          # ## %######################################################%## # Fit some models # require(gamlss) # m1 <- gamlss::fitDist(some_sp$ind_ha, type=\"realline\") # m1$fits # m1$failed # # m1 <- gamlss(ind_ha ~ pb(elevation) + pb(sand) + pb(bio3) + pb(bio12), family=NO, data=some_sp) # choosen_dist <- gamlss::chooseDist(m1, parallel=\"snow\", ncpus=4, type=\"realAll\")  mgam <- fit_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   predictors_f = \"eco\",   partition = \".part\",   distribution = gamlss.dist::NO() )  mraf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\", )  mgbm <- fit_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\",   distribution =   )   ## %######################################################%## #                                                          # ####            ' ####      Predict models              #### #                                                          # ## %######################################################%##  # adm_predict can be used for predict one or more models fitted with fit_ or tune_ functions  # a single model ind_p <- sdm_predict(   models = mglm,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # a list of models list_p <- sdm_predict(   models = list(mglm, mraf, mgbm),   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # Predict an ensemble model # (only is possilbe use one fit_ensemble) ensemble_p <- sdm_predict(   models = mensemble,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # Predict an ensemble of small models # (only is possible to use one ensemble of small models) small_p <- sdm_predict(   models = msmall,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  ## %######################################################%## #                                                          # ####              Predict model using chunks            #### #                                                          # ## %######################################################%## # Predicting models in chunks helps reduce memory requirements in # cases where models are predicted for large scales and high resolution  ind_p <- sdm_predict(   models = mglm,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL,   nchunk = 4 ) }"},{"path":"https://xx.github.io/adm/reference/adm_summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge model performance tables — adm_summarize","title":"Merge model performance tables — adm_summarize","text":"Merge model performance tables","code":""},{"path":"https://xx.github.io/adm/reference/adm_summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge model performance tables — adm_summarize","text":"","code":"adm_summarize(models)"},{"path":"https://xx.github.io/adm/reference/adm_summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge model performance tables — adm_summarize","text":"models list. list single several models fitted fit_ tune_ functions. Usage models = list(mod1, mod2, mod3)","code":""},{"path":"https://xx.github.io/adm/reference/adm_summarize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge model performance tables — adm_summarize","text":"tibble object combined model performance input models. Models fit tune include model performance best hyperparameters.","code":""},{"path":"https://xx.github.io/adm/reference/adm_summarize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge model performance tables — adm_summarize","text":"","code":"if (FALSE) { require(dplyr) require(terra)  data(\"sppabund\") envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Species abundance data, coordinates, and partition some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y, .part) some_sp  # Extract data some_sp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar   )  # Fit RAF m_raf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\", )  # Fit SVM m_svm <- fit_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\" )  # XGB m_xbg <- fit_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\" )   perf <- adm_summarize(list(m_svm, m_raf, m_xbg))  perf }"},{"path":"https://xx.github.io/adm/reference/adm_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs data transformation on a variable based on the specified method. — adm_transform","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"function transforms data tibble SpatRaster object method specified available methods \"01\", \"zscore\", \"log\", \"round\".","code":""},{"path":"https://xx.github.io/adm/reference/adm_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"","code":"adm_transform(data, variable, method, inverse = FALSE, t_terms = NULL)"},{"path":"https://xx.github.io/adm/reference/adm_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"data data.frame, tibble, SpatRaster containing data. variable character string specifying variable (column) transformed. method character string specifying method used transformation. Available methods \"01\", \"zscore\", \"log\", \"round.\" \"01\", scales variable 0 1 using formula (x - min(x)) (max(x) - min(x)). \"zscore\", standardizes variable subtracting mean dividing standard deviation. \"log\", applies natural logarithm transformation variable. \"log1\", sums 1 applies natural logarithm transformation variable. \"round\", rounds variable's values nearest whole numbers. inverse logical. Invert transformation? t_terms vector. c(,b): \"01\", = min(x), b = max(x). \"zscore\", = mean(x), b = sd(x). \"log\" \"log1, needed. invert \"round\" transformations.","code":""},{"path":"https://xx.github.io/adm/reference/adm_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"data.frame tibble transformed variable added new column. new column's name original variable name followed underscore method name.","code":""},{"path":"https://xx.github.io/adm/reference/adm_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"","code":"if (FALSE) { require(dplyr) require(terra)  # Select data for a single species data(\"sppabund\") some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)[[\"bio12\"]]  # Transform tabular data ## Transform abundance data to 0-1 some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"01\" ) some_sp_2  ## Transform abundance data z-score some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"zscore\" ) some_sp_2  ## Transform abundance data log some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"log\" ) some_sp_2  ## Round abundance data some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"round\" ) some_sp_2  # Tranform raster data ## Transform to 0-1 envar_2 <- adm_transform(   data = envar,   variable = \"bio12\",   method = \"01\" ) envar_2  ## Transform z-score envar_2 <- adm_transform(   data = envar,   variable = \"bio12\",   method = \"zscore\" ) envar_2  ## Transform log envar_2 <- adm_transform(   data = envar,   variable = \"bio12\",   method = \"log\" ) envar_2  ## Round envar_2 <- adm_transform(   data = envar,   variable = \"bio12\",   method = \"round\" ) envar_2  # Invert transformation ## Invert 01 tranformation some_sp_transformed <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"01\" ) some_sp_transformed  some_sp_inverted <- adm_transform(   data = some_sp_transformed,   variable = \"ind_ha_01\",   method = \"01\",   inverse = TRUE,   t_terms = c(     a = min(some_sp[[\"ind_ha\"]]),     b = max(some_sp[[\"ind_ha\"]])   ) ) some_sp_inverted  ## Invert z-score tranformation some_sp_transformed <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"zscore\" ) some_sp_transformed  some_sp_inverted <- adm_transform(   data = some_sp_transformed,   variable = \"ind_ha_zscore\",   method = \"zscore\",   inverse = TRUE,   t_terms = c(     a = mean(some_sp[[\"ind_ha\"]]),     b = sd(some_sp[[\"ind_ha\"]])   ) ) some_sp_inverted  ## Invert log and log1 some_sp_transformed <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"log\" ) some_sp_transformed  some_sp_inverted <- adm_transform(   data = some_sp_transformed,   variable = \"ind_ha_log\",   method = \"log\",   inverse = TRUE ) some_sp_inverted  some_sp_transformed <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"log1\" ) some_sp_transformed  some_sp_inverted <- adm_transform(   data = some_sp_transformed,   variable = \"ind_ha_log1\",   method = \"log1\",   inverse = TRUE ) some_sp_inverted  ## To invert raster, is the same process ## Example: envar_transformed <- adm_transform(   data = envar,   variable = \"bio12\",   method = \"01\" ) envar_transformed  envar_inverted <- adm_transform(   data = envar_transformed,   variable = \"bio12_01\",   method = \"01\",   inverse = TRUE,   t_terms = c(     a = terra::global(envar, min, na.rm = T),     b = terra::global(envar, max, na.rm = T)   ) ) envar_inverted }"},{"path":"https://xx.github.io/adm/reference/balance_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Balance database at a given absence-presence ratio — balance_dataset","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"function balances given database based specified ratio absence presence. randomly removes excess absence database achieve specified ratio. function interprets absence data abundance equal zero.","code":""},{"path":"https://xx.github.io/adm/reference/balance_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"","code":"balance_dataset(data, response, absence_ratio)"},{"path":"https://xx.github.io/adm/reference/balance_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"data data.frame tibble. Database contains columns abundance. response string. name column `data` representing response variable. Note absence interpreted data abundance equal zero. Usage response = \"ind_ha\" absence_ratio numeric. desired ratio presence absence response column. E.g., set 1 function remove absence number presence. set 1.5, function remove absence 1.5 times number presence. Usage absence_ratio = 0.5","code":""},{"path":"https://xx.github.io/adm/reference/balance_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"Returns balanced data.frame tibble absence-presence ratio response column equal absence_ratio","code":""},{"path":"https://xx.github.io/adm/reference/balance_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"","code":"if (FALSE) { require(dplyr)  data(\"sppabund\") some_sp <- sppabund %>%   dplyr::filter(species == \"Species three\") %>%   dplyr::select(species, ind_ha, x, y)  table(some_sp$ind_ha > 0) # Note that the dataset is almost balanced # However, as an example, let's assume that we want to reduce # the number of absences half of the number of presences  some_sp_2 <- balance_dataset(   data = some_sp,   response = \"ind_ha\",   absence_ratio = 0.5 )  table(some_sp$ind_ha > 0) table(some_sp_2$ind_ha > 0) }"},{"path":"https://xx.github.io/adm/reference/cnn_make_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates sample data for Convolutional Neural Network — cnn_make_samples","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"function creates array input images associated responses can utilized train Convolutional Neural Network.","code":""},{"path":"https://xx.github.io/adm/reference/cnn_make_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"","code":"cnn_make_samples(   data,   x,   y,   response,   raster,   raster_padding = FALSE,   padding_method = NULL,   size = 5 )"},{"path":"https://xx.github.io/adm/reference/cnn_make_samples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"data data.frame tibble. Database includes longitude, latitude, response columns. x string. Specifying name column longitude data. y string. Specifying name column latitude data. response string. Specifying name column response. raster SpatRaster. Raster predictor data cropped. raster_padding logical. TRUE, raster padded cropping extends beyond boundaries. Useful ensuring focal cells size output even edges raster. Default FALSE padding_method string NULL. Method used padding raster raster_padding TRUE. Options \"mean\", \"median\", \"zero\". Ignored raster_padding FALSE. Default NULL size numeric. Size cropped raster, number o cell direction focal cell","code":""},{"path":"https://xx.github.io/adm/reference/cnn_make_samples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"list two elements - 'predict' (list input images) 'response' (response values). element 'predictors' list array representing cropped image input raster.","code":""},{"path":"https://xx.github.io/adm/reference/cnn_make_samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"","code":"if (FALSE) { require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  cnn_samples <- cnn_make_samples(   data = some_sp,   x = \"x\", # x coordinates for each point   y = \"y\", # y coordinates for each point   response = \"ind_ha\",   raster = envar[[c(\"bio12\", \"sand\", \"elevation\")]],   size = 5 # how many pixels from point to border? )  length(cnn_samples$predictors) #  938 matrix sets dim(cnn_samples$predictors[[1]]) # three 11x11 channels cnn_samples$predictors[[1]] # representing predictor variables rast(cnn_samples$predictors[[1]]) %>% plot()  cnn_samples$response[[1]] # linked to a label }"},{"path":"https://xx.github.io/adm/reference/cretusa_data.html","id":null,"dir":"Reference","previous_headings":"","what":"A data set containing abundance of Cynophalla retusa. — cretusa_data","title":"A data set containing abundance of Cynophalla retusa. — cretusa_data","text":"Cynophalla retusa (Griseb.) Cornejo & Iltis (Capparaceae) shrub native northeastern Argentina, Paraguay, Bolivia, central Brazil.","code":""},{"path":"https://xx.github.io/adm/reference/cretusa_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A data set containing abundance of Cynophalla retusa. — cretusa_data","text":"","code":"cretusa_data"},{"path":"https://xx.github.io/adm/reference/cretusa_data.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A data set containing abundance of Cynophalla retusa. — cretusa_data","text":"tibble 366 rows 6 variables: species species names ind_ha species abundance expressed individuals per hectare x longitude species occurrences y latitude species occurrences .part partition folds","code":""},{"path":"https://xx.github.io/adm/reference/cretusa_data.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A data set containing abundance of Cynophalla retusa. — cretusa_data","text":"","code":"if (FALSE) { require(dplyr) data(\"cretusa_data\") cretusa_data }"},{"path":"https://xx.github.io/adm/reference/cretusa_predictors.html","id":null,"dir":"Reference","previous_headings":"","what":"Raster with Principal Component — cretusa_predictors","title":"Raster with Principal Component — cretusa_predictors","text":"raster principal components derived principal component analysis based climatic (Chelsa: chelsa-climate.org) edaphic (SoilGrids: soilgrids.org) variables.","code":""},{"path":"https://xx.github.io/adm/reference/cretusa_predictors.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Raster with Principal Component — cretusa_predictors","text":"","code":"cretusa_predictors()"},{"path":"https://xx.github.io/adm/reference/cretusa_predictors.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Raster with Principal Component — cretusa_predictors","text":"raster tif format first principal components.","code":""},{"path":"https://xx.github.io/adm/reference/cretusa_predictors.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Raster with Principal Component — cretusa_predictors","text":"","code":"if (FALSE) { require(terra) envar <- system.file(\"external/cretusa_predictors.tif\", package = \"adm\") envar <- terra::rast(envar) plot(envar) }"},{"path":"https://xx.github.io/adm/reference/croppin_hood.html","id":null,"dir":"Reference","previous_headings":"","what":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"Crop rasters single spatial point. Function used internally construct Convolutional Neural Networks","code":""},{"path":"https://xx.github.io/adm/reference/croppin_hood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"","code":"croppin_hood(   occ,   x,   y,   raster,   size,   raster_padding = FALSE,   padding_method = NULL )"},{"path":"https://xx.github.io/adm/reference/croppin_hood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"occ tibble data.frame. Database response, predictors, partition values x character. Column name spatial x coordinates y character. Column name spatial y coordinates raster SpatRaster. Raster environmental variables. size numeric. Size cropped raster, number o cell direction focal cell raster_padding logical. TRUE, raster padded cropping extends beyond boundaries. Useful ensuring focal cells size output even edges raster. Default FALSE padding_method string NULL. Method used padding raster raster_padding TRUE. Options \"mean\", \"median\", \"zero\". Ignored raster_padding FALSE. Default NULL","code":""},{"path":"https://xx.github.io/adm/reference/croppin_hood.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"SpatRaster. Croped raster","code":""},{"path":"https://xx.github.io/adm/reference/croppin_hood.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"","code":"if (FALSE) { require(terra)  # Datasbase with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   filter(species == \"Species three\")  # Raster data with environmental variables envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # sampl_r <- croppin_hood(occ = some_sp[1, ], x = \"x\", y = \"y\", raster = envar, size = 5) plot(sampl_r) plot(sampl_r[[1]]) points(some_sp[1, c(\"x\", \"y\")], pch = 19) }"},{"path":"https://xx.github.io/adm/reference/data_abund_bpdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"Calculate data construct bivariate partial dependence two predictor set","code":""},{"path":"https://xx.github.io/adm/reference/data_abund_bpdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"","code":"data_abund_bpdp(   model,   predictors,   resolution = 50,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   training_boundaries = NULL,   projection_data = NULL,   sample_size = NULL,   training_raster = NULL,   x_coord = NULL,   y_coord = NULL )"},{"path":"https://xx.github.io/adm/reference/data_abund_bpdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"model object returned fit_abund tune_abund family functions predictors character. Vector two predictor name(s) plot. NULL predictors plotted. Default NULL resolution numeric. Number equally spaced points predict continuous predictors. Default 50 training_data data.frame tibble. Database response (0,1) predictor values used fit model. Default NULL invert_transform logical. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Default NULL response_name character. Name response variable. Default NULL training_boundaries character. Plot training conditions boundaries based training data (.e., presences, presences absences, etc). training_boundaries = \"convexh\", function delimit training environmental region based convex-hull. training_boundaries = \"rectangle\", function delimit training environmental region based four straight lines. used methods necessary provide data training_data argument. NULL predictors used. Default NULL. projection_data SpatRaster. Raster layer environmental variables used model projection. Default NULL sample_size vector. CNN . vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) training_raster terra SpatRaster object. CNN . raster containing predictor variables used tune_abund_cnn fit_abund_cnn. x_coord character. CNN . name column containing longitude information observation. y_coord character. CNN . name column containing latitude information observation.","code":""},{"path":"https://xx.github.io/adm/reference/data_abund_bpdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"list two tibbles \"pdpdata\" \"resid\". pdpdata: data construct partial dependence bivariate plot, first two column includes values selected environmental variables, third column predicted suitability. training_boundaries: data plot boundaries training data.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/data_abund_bpdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"","code":"if (FALSE) { require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Prepare data for Bivariate Partial Dependence Plots bpdp_data <- data_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   resolution = 25,   training_data = some_sp,   response_name = \"Abundance\",   projection_data = envar,   training_boundaries = \"convexh\" )  bpdp_data }"},{"path":"https://xx.github.io/adm/reference/data_abund_pdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate data to construct partial dependence plots — data_abund_pdp","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"Calculate data construct partial dependence plots","code":""},{"path":"https://xx.github.io/adm/reference/data_abund_pdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"","code":"data_abund_pdp(   model,   predictors,   resolution = 50,   resid = FALSE,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   projection_data = NULL,   sample_size = c(11, 11),   training_raster = NULL,   x_coord = NULL,   y_coord = NULL )"},{"path":"https://xx.github.io/adm/reference/data_abund_pdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"model object returned fit_abund tune_abund family functions predictors character. Vector two predictor name(s) plot. NULL predictors plotted. Default NULL resolution numeric. Number equally spaced points predict continuous predictors. Default 50 resid logical. Calculate residuals based training data. Default FALSE training_data data.frame. Database response predictor values used fit model. Default NULL. Required GLM, GAM, DNN, NET, RAF, SVM models invert_transform vector. vector containing method terms invert transformation response variable. Useful cases response variable transformed one method adm_transform. Usage: \"01\": invert_transform = c(method = \"01\", = min(x), b = max(x)) \"zscore\": invert_transform = c(method = \"zscore\", = mean(x), b = sd(x)) \"log\" \"log1: needed. invert \"round\" transformations. Default NULL response_name character. Name response variable. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL sample_size vector. CNN . vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) training_raster terra SpatRaster object. CNN . raster containing predictor variables used tune_abund_cnn fit_abund_cnn. x_coord character. CNN . name column containing longitude information observation. y_coord character. CNN . name column containing latitude information observation.","code":""},{"path":"https://xx.github.io/adm/reference/data_abund_pdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"list two tibbles \"pdpdata\" \"resid\". pdpdata: data construct partial dependence plots, first column includes values selected environmental variable, second column predicted suitability, third  column range type, two values Training Projecting, referring suitability  calculated within outside range training conditions. Third column returned  \"projection_data\" argument used resid: data plot residuals. first column includes values selected environmental  variable second column predicted suitability.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/data_abund_pdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"","code":"if (FALSE) { require(dplyr) require(terra)  # Load  data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast()  data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Prepare data for Partial Dependence Plots pdp_data <- data_abund_pdp(   model = mglm,   predictors = \"bio12\",   resolution = 25,   resid = TRUE,   training_data = some_sp,   response_name = \"Abundance\",   projection_data = envar )  pdp_data }"},{"path":"https://xx.github.io/adm/reference/envar.html","id":null,"dir":"Reference","previous_headings":"","what":"Raster with environmental data — envar","title":"Raster with environmental data — envar","text":"raster climatic (Chelsa: chelsa-climate.org), edaphic (SoilGrids: soilgrids.org), ecoregion data (WWF https://www.worldwildlife.org/publications/terrestrial-ecoregions---world).","code":""},{"path":"https://xx.github.io/adm/reference/envar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Raster with environmental data — envar","text":"","code":"envar()"},{"path":"https://xx.github.io/adm/reference/envar.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Raster with environmental data — envar","text":"raster tif format following variables. bio1 Annual mean temperature bio12 Annual precipitation bio15 Precipitation seasonality bio3 Isothermality cfvo Volumetric fraction coarse fragments elevation Elevation sand Soil sand content eco Terrestrial ecoregion","code":""},{"path":"https://xx.github.io/adm/reference/envar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Raster with environmental data — envar","text":"","code":"if (FALSE) { require(terra) envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar) plot(envar) }"},{"path":"https://xx.github.io/adm/reference/family_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Select probability distributions for GAM and GLM — family_selector","title":"Select probability distributions for GAM and GLM — family_selector","text":"Select probability distribution available gamlss.dist suited given response variables (e.g., count, zero-inflated) used fit GAM GLM models. See gamlss.family details.","code":""},{"path":"https://xx.github.io/adm/reference/family_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select probability distributions for GAM and GLM — family_selector","text":"","code":"family_selector(data, response)"},{"path":"https://xx.github.io/adm/reference/family_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select probability distributions for GAM and GLM — family_selector","text":"data data.frame tibble. Database species abundance response character. Column name species abundance","code":""},{"path":"https://xx.github.io/adm/reference/family_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select probability distributions for GAM and GLM — family_selector","text":"tibble family_name, family_call, range, discrete columns (family distribution discrete)","code":""},{"path":"https://xx.github.io/adm/reference/family_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select probability distributions for GAM and GLM — family_selector","text":"","code":"if (FALSE) { data(sppabund)  family_selector(data = sppabund, response = \"ind_ha\") }"},{"path":"https://xx.github.io/adm/reference/fit_abund_cnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"function used fit convolutional neural network (CNN) model abundance.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_cnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"","code":"fit_abund_cnn(   data,   response,   predictors,   predictors_f = NULL,   x,   y,   rasters,   sample_size,   partition,   predict_part = FALSE,   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = NULL,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_cnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") x character. name column containing longitude information observation. y character. name column containing latitude information observation. rasters terra SpatRaster object. raster containing predictor variables cropped around observation. sample_size numeric. vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE learning_rate numeric. size step taken optimization process. Default = 0.01 n_epochs numeric. Maximum number times learning algorithm work training set. Default = 10 batch_size numeric. batch subset training set used single iteration training process. size batch referred batch size. Default = 32 validation_patience numerical. integer indicating number epochs without loss improvement tolerated algorithm validation process. patience limit exceeded, training ends. Default 2 fitting_patience numerical. validation_patience, final model fitting process. Default 5 custom_architecture Torch nn_module_generator object. neural network architecture used instead internal default one. Default NULL verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_cnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_cnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Generate an architecture cnn_arch <- generate_cnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = T )  # Fit a CNN model mcnn <- fit_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   x = \"x\",   y = \"y\",   rasters = envar,   sample_size = c(11, 11),   learning_rate = 0.01,   n_epochs = 100,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = cnn_arch,   verbose = TRUE,   predict_part = TRUE )  mcnn }"},{"path":"https://xx.github.io/adm/reference/fit_abund_dnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Deep Neural Network model — fit_abund_dnn","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"Fit validate Deep Neural Network model","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_dnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"","code":"fit_abund_dnn(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = NULL,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_dnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE learning_rate numeric. size step taken optimization process. Default = 0.01 n_epochs numeric. Max number times learning algorithm work training set. Default = 10 batch_size numeric. batch subset training set used single iteration training process. size batch referred batch size. Default = 32 validation_patience numerical. integer indicating number epochs without loss improvement tolerated algorithm validation process. patience limit exceeded, training ends. Default 2 fitting_patience numerical. validation_patience, final model fitting process. Default 5 custom_architecture Torch nn_module_generator object generate_dnn_architecture output. neural network architecture used instead internal default one. Default NULL verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_dnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_dnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate a architecture dnn_arch <- generate_dnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   number_of_hidden_layers = 3,   hidden_layers_size = c(8, 16, 8),   batch_norm = TRUE )  # Fit a NET model mdnn <- fit_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = dnn_arch,   verbose = TRUE,   predict_part = TRUE )  mdnn }"},{"path":"https://xx.github.io/adm/reference/fit_abund_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Additive Models — fit_abund_gam","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"Fit validate Generalized Additive Models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"","code":"fit_abund_gam(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   distribution = NULL,   inter = \"automatic\",   verbose = TRUE,   control_gamlss = gamlss::gamlss.control(trace = FALSE) )"},{"path":"https://xx.github.io/adm/reference/fit_abund_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE distribution character. string specifying distribution used. See gamlss.family documentation details. Use distribution = gamlss.dist::(). Default NULL inter integer. Number knots x-axis. Default \"automatic\" verbose logical. FALSE, disables console messages. Default TRUE control_gamlss function. control parameters outer iterations algorithm gamlss See gamlss.control documentation details. Default gamlss.control()","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"list object : model: \"gamlss\" class object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"","code":"if (FALSE) { require(terra) require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions family_selector(data = some_sp, response = \"ind_ha\") %>% tail()  # Fit a GAM model mgam <- fit_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   sigma_formula = ~ elevation + bio3 + bio12,   predictors_f = NULL,   partition = \".part\",   distribution = gamlss.dist::ZAGA() )  mgam }"},{"path":"https://xx.github.io/adm/reference/fit_abund_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"Fit validate Generalized Boosted Regression models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"","code":"fit_abund_gbm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   distribution,   n.trees = 100,   interaction.depth = 5,   n.minobsinnode = 5,   shrinkage = 0.1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_gbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE distribution character. string specifying distribution used. See gbm::gbm documentation details. n.trees integer. total number trees fit. interaction.depth integer. maximum depth tree. Default 5 n.minobsinnode integer. minimum number observations terminal nodes trees. Default 5 shrinkage numeric. learning rate algorithm. Default 0.1 verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"list object : model: \"gbm\" class object gbm package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a GBM model mgbm <- fit_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"gaussian\",   n.trees = 100,   interaction.depth = 5,   n.minobsinnode = 5,   shrinkage = 0.1,   predict_part = TRUE )  mgbm }"},{"path":"https://xx.github.io/adm/reference/fit_abund_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Linear Models — fit_abund_glm","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"Fit validate Generalized Linear Models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"","code":"fit_abund_glm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   distribution = NULL,   poly = 0,   inter_order = 0,   control_gamlss = gamlss::gamlss.control(trace = FALSE),   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. distribution character. string specifying distribution used. See gamlss.family documentation details. Use distribution = gamlss.dist::(). Default NULL poly integer >= 2. used values >= 2 model use polynomials continuous variables (.e. used predictors argument). Default 0. inter_order integer >= 0. interaction order explanatory variables. Default 0. control_gamlss function. control parameters outer iterations algorithm gamlss See gamlss.control documentation details. Default gamlss.control() verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"list object : model: \"gamlss\" class object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"","code":"if (FALSE) { require(terra) require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions family_selector(data = some_sp, response = \"ind_ha\") %>% tail()  # Fit a GLM model glm_1 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 0,   inter_order = 0,   predict_part = TRUE )  glm_1  # Using second order polynomials and first order interaction terms glm_2 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 2,   inter_order = 1,   predict_part = TRUE )  glm_2  # Using third order polynomials and second order interaction terms glm_3 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 3,   inter_order = 2,   predict_part = TRUE )  glm_3  # Setting formulas for different distribution parameters glm_4 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   fit_formula = ind_ha ~ bio12 + elevation + sand + eco,   sigma_formula = ind_ha ~ bio12 + elevation + sand,   poly = 0,   inter_order = 0,   predict_part = TRUE )  glm_4 }"},{"path":"https://xx.github.io/adm/reference/fit_abund_net.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Artificial Neural Network models — fit_abund_net","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"Fit validate Artificial Neural Network models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_net.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"","code":"fit_abund_net(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   size,   decay = 0,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_net.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. size numerical. size hidden layer. decay numerial. Value weight decay. Default 0. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_net.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_net.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a NET model mnet <- fit_abund_net(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   size = 32,   decay = 0.1,   predict_part = TRUE )  mnet }"},{"path":"https://xx.github.io/adm/reference/fit_abund_raf.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Random Forests models — fit_abund_raf","title":"Fit and validate Random Forests models — fit_abund_raf","text":"Fit validate Random Forests models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_raf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Random Forests models — fit_abund_raf","text":"","code":"fit_abund_raf(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   mtry = length(c(predictors, predictors_f))/3,   ntree = 500,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_raf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Random Forests models — fit_abund_raf","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. mtry numeric. Number variables randomly sampled candidates split. Default (length(c(predictors, predictors_f))/3) ntree numeric. Number trees grow. set small number, ensure every input row gets predicted least times. Default 500 verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_raf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Random Forests models — fit_abund_raf","text":"list object : model: \"randomForest\" class object randomForest package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/fit_abund_raf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Random Forests models — fit_abund_raf","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a RAF model mraf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   mtry = 3,   ntree = 500,   predict_part = TRUE )  mraf }"},{"path":"https://xx.github.io/adm/reference/fit_abund_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Support Vector Machine models — fit_abund_svm","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"Fit validate Support Vector Machine models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"","code":"fit_abund_svm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   kernel = \"rbfdot\",   sigma = \"automatic\",   C = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. kernel character. string defining kernel used algorithm. Default \"rbfdot\". sigma numeric character. Either \"automatic\" (recommended) inverse kernel width Radial Basis kernel function \"rbfdot\" Laplacian kernel \"laplacedot\". Default \"automatic\". C numeric. Cost constraints violation. Default 1. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_svm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_svm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a SVM model msvm <- fit_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   kernel = \"rbfdot\",   sigma = \"automatic\",   C = 1,   predict_part = TRUE )  msvm }"},{"path":"https://xx.github.io/adm/reference/fit_abund_xgb.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"Fit validate Extreme Gradient Boosting models","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_xgb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"","code":"fit_abund_xgb(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   nrounds = 100,   max_depth = 5,   eta = 0.1,   gamma = 1,   colsample_bytree = 1,   min_child_weight = 1,   subsample = 0.5,   objective = \"reg:squarederror\",   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/fit_abund_xgb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE. nrounds integer. Max number boosting iterations. Default 100. max_depth integer. maximum depth tree. Default 5 eta numeric. learning rate algorithm. Default 0.1 gamma numeric. Minimum loss reduction required make partition leaf node tree. Default 1. colsample_bytree numeric. Subsample ratio columns constructing tree. Default 1. min_child_weight numeric. Minimum sum instance weight needed child. Default 1. subsample numeric. Subsample ratio training instance. Default 0.5. objective character. learning task corresponding learning objective. Default \"reg:squarederror\", regression squared loss. verbose logical. FALSE, disables console messages. Default TRUE.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_xgb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"list object : model: \"xgb.Booster\" class object xgboost package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"https://xx.github.io/adm/reference/fit_abund_xgb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a XGB model mxgb <- fit_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   nrounds = 200,   max_depth = 5,   eta = 0.1,   gamma = 1,   colsample_bytree = 0.7,   min_child_weight = 2,   subsample = 0.3,   objective = \"reg:squarederror\",   predict_part = TRUE )  mxgb }"},{"path":"https://xx.github.io/adm/reference/generate_arch_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"function generates list architectures either Deep Neural Netwokd (DNN) Convolutional Neural Network (CNN).","code":""},{"path":"https://xx.github.io/adm/reference/generate_arch_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"","code":"generate_arch_list(   type,   number_of_features,   number_of_outputs,   n_layers = c(1, 2),   n_neurons = c(7),   sample_size = c(11, 11),   number_of_fc_layers = 1,   fc_layers_size = c(14),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   pooling = NULL,   batch_norm = TRUE,   dropout = 0 )"},{"path":"https://xx.github.io/adm/reference/generate_arch_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"type string. Specifies type network. valid inputs \"dnn\" \"cnn\". number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. n_layers numeric. Vector specifies number layers networks. Default value 1 2. n_neurons vector. Specifies number neurons layer. Default  7. sample_size vector. Specifies size. Default c(11, 11) number_of_fc_layers numeric. Specifies number fully connected layers. Default 1. fc_layers_size vector. Specifies size fully connected layers. Default 14. conv_layers_kernel numeric. Specifies kernel size layers. Default 3. conv_layers_stride numeric. Specifies stride convolutional layers. Default 1. conv_layers_padding numeric. Specifies padding convolutional layers. Default 0. pooling numeric. Specifies 2D average pooling kernel size. Default NULL batch_norm logical. Specifies whether batch normalization included architecture. Default TRUE. dropout Numeric. probability (p) randomly zeroing elements input tensor training prevent overfitting. Must 0 (dropout) 1 (inputs zeroed). Default 0 (dropout).","code":""},{"path":"https://xx.github.io/adm/reference/generate_arch_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"list containing: arch_list: list generated architectures. arch_dict: list architecture dictionaries.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/generate_arch_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"","code":"if (FALSE) { # Generating architectures for DNN, using batch normalization and dropout dnn_archs <- generate_arch_list(   type = \"dnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = c(2, 3, 4),   n_neurons = c(8, 16, 32, 64),   batch_norm = TRUE,   dropout = 0.2 )  dnn_archs$arch_dict # Matrices describing the networks length(dnn_archs$arch_list) # Generated 336 DNN architectures  # Generating architectures for CNN, using batch normalization, dropout and average pooling # Note that arguments meaning change with the context  cnn_archs <- generate_arch_list(   type = \"cnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = c(2, 3, 4), # now convolutional layers   n_neurons = c(8, 16, 32, 64),   sample_size = c(11, 11),   number_of_fc_layers = c(2, 4), # fully connected layers   fc_layers_size = c(16, 8),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   pooling = 1,   batch_norm = TRUE,   dropout = 0.2 )  cnn_archs$arch_dict # Matrices describing the networks length(cnn_archs$arch_list) # Generated 6720 CNN architectures  # The list size can be easily and greatly reduced with select_arch_list  dnn_archs_redux <- dnn_archs %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(dnn_archs_redux$arch_list) # from 336 to 29 architectures  cnn_archs_redux <- cnn_archs %>% select_arch_list(   type = c(\"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(cnn_archs_redux$arch_list) # from 6720 to 77 architectures }"},{"path":"https://xx.github.io/adm/reference/generate_cnn_architecture.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"Generate architectures Convolutional Neural Network","code":""},{"path":"https://xx.github.io/adm/reference/generate_cnn_architecture.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"","code":"generate_cnn_architecture(   number_of_features = 7,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = FALSE )"},{"path":"https://xx.github.io/adm/reference/generate_cnn_architecture.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. sample_size vector. Specifies size. Default c(11, 11) number_of_conv_layers numeric. Specifies number convolutional layers. Default 2. conv_layers_size numeric. size convolutional layers. Default c(14, 28). conv_layers_kernel numeric. Specifies kernel size layers. Default 3. conv_layers_stride numeric. Specifies stride convolutional layers. Default 1. conv_layers_padding numeric. Specifies padding convolutional layers. Default 0. number_of_fc_layers numeric. Specifies number fully connected layers. Default 1. fc_layers_size vector. Specifies size fully connected layers. Default 14. pooling numeric. Specifies 2D average pooling kernel size. Default NULL batch_norm logical. Specifies whether batch normalization included architecture. Default TRUE. dropout Numeric. probability (p) randomly zeroing elements input tensor training prevent overfitting. Must 0 (dropout) 1 (inputs zeroed). Default 0 (dropout). verbose logical. Specifies whether architecture printed. Default FALSE.","code":""},{"path":"https://xx.github.io/adm/reference/generate_cnn_architecture.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"list containing: net: instantiated torch neural net. arch: string R expression instantiate neural network. arch_dict: list matrix describing architecture structure.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/generate_cnn_architecture.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"","code":"if (FALSE) { # Generate a Conclutional Neural Network with: cnn_arch <- generate_cnn_architecture(   number_of_features = 7, # seven input variables   number_of_outputs = 1, # one output   sample_size = c(11, 11), # image dimensions   number_of_conv_layers = 2, # two convolutional layers between input and output   conv_layers_size = c(14, 28), # of this size, respectively   conv_layers_kernel = 3, # with a 3 pixels kernel   conv_layers_stride = 1, # walking 1 pixel at a time   conv_layers_padding = 1, # with 1 pixel of padding   number_of_fc_layers = 1, # followed by one fully connected layer   fc_layers_size = c(28), # with 28 neurons   pooling = NULL, # without average pooling   batch_norm = TRUE, # with batch normalization   dropout = 0 # and without dropout )  cnn_arch$net() # a torch net cnn_arch$arch %>% cat() # the torch code to create it cnn_arch$arch_dict # and a quick description of its structure }"},{"path":"https://xx.github.io/adm/reference/generate_dnn_architecture.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architectures for Deep Neural Network — generate_dnn_architecture","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"Generate architectures Deep Neural Network","code":""},{"path":"https://xx.github.io/adm/reference/generate_dnn_architecture.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"","code":"generate_dnn_architecture(   number_of_features = 7,   number_of_outputs = 1,   number_of_hidden_layers = 2,   hidden_layers_size = c(14, 7),   batch_norm = TRUE,   dropout = 0,   verbose = FALSE )"},{"path":"https://xx.github.io/adm/reference/generate_dnn_architecture.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. number_of_hidden_layers numeric. Number hidden layers neural network. Default 2. hidden_layers_size numeric vector. Size hidden layer neural network. Default c(14, 7). batch_norm logical. Whether include batch normalization layers. Default TRUE. dropout logical. Specifies whether dropout included architecture. Default FALSE. verbose logical. Whether print architecture. Default FALSE.","code":""},{"path":"https://xx.github.io/adm/reference/generate_dnn_architecture.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"list containing: net: instantiated torch neural net. arch: string R expression instantiate neural network. arch_dict: list matrix describing architecture structure.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/generate_dnn_architecture.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"","code":"if (FALSE) { # Generate a Deep Neural Network with: dnn_arch <- generate_dnn_architecture(   number_of_features = 8, # eight input variables   number_of_outputs = 1, # one output   number_of_hidden_layers = 5, # five layers between input and output   hidden_layers_size = c(8, 16, 32, 16, 8), # of this size, respectively   batch_norm = TRUE, # with batch normalization   dropout = 0, # without dropout )  dnn_arch$net() # a torch net dnn_arch$arch %>% cat() # the torch code to create it dnn_arch$arch_dict # and a quick description of its structure }"},{"path":"https://xx.github.io/adm/reference/model_selection.html","id":null,"dir":"Reference","previous_headings":"","what":"Best hyperparameter selection — model_selection","title":"Best hyperparameter selection — model_selection","text":"Best hyperparameter selection","code":""},{"path":"https://xx.github.io/adm/reference/model_selection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Best hyperparameter selection — model_selection","text":"","code":"model_selection(hyper_combinations, metrics)"},{"path":"https://xx.github.io/adm/reference/model_selection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Best hyperparameter selection — model_selection","text":"hyper_combinations tibble data.frame. hyperparameter combinations performance metrics metrics character. performance metrics considered","code":""},{"path":"https://xx.github.io/adm/reference/model_selection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Best hyperparameter selection — model_selection","text":"list containing: optimal_combination: tibble optimal hyperparameter combination. all_combinations: combinations.","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_bpdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"Create bivariate partial dependence plots explore marginal effect predictors modeled abundance","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_bpdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"","code":"p_abund_bpdp(   model,   predictors = NULL,   resolution = 50,   training_data = NULL,   projection_data = NULL,   training_boundaries = NULL,   invert_transform = NULL,   response_name = NULL,   color_gradient = c(\"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\",     \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\"),   color_training_boundaries = \"white\",   set_max = NULL,   set_min = NULL,   theme = ggplot2::theme_classic(),   sample_size = NULL,   training_raster = NULL,   x_coord = NULL,   y_coord = NULL )"},{"path":"https://xx.github.io/adm/reference/p_abund_bpdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"model model object found first element list returned function fit_abund_ tune_abund_ function families predictors character. Vector predictor name(s) calculate partial dependence plots. NULL predictors used. Default NULL resolution numeric. Number equally spaced points predict abundance values continuous predictors. Default 50 training_data data.frame tibble. Database response predictor values used fit model. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL training_boundaries character. Plot training conditions boundaries based training data. training_boundaries = \"convexh\", function delimit training environmental region based convex-hull. training_boundaries = \"rectangle\", function delimit training environmental region based four straight lines. used methods necessary provide data training_data argument.NULL predictors used. Default NULL. invert_transform logical. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Default NULL response_name character. Name response variable. Default NULL color_gradient character. Vector gradient colors. Default c(\"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\", \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\") color_training_boundaries character. vector one color used color points residuals, Default \"white\" set_max numeric. Set maximum abundance value plot set_min numeric. Set minimum abundance value plot theme ggplot2 theme. Default ggplot2::theme_classic() sample_size vector. CNN . vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) training_raster terra SpatRaster object. CNN . raster containing predictor variables used tune_abund_cnn fit_abund_cnn. x_coord character. CNN . name column containing longitude information observation. y_coord character. CNN . name column containing latitude information observation.","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_bpdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"ggplot object","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/p_abund_bpdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"","code":"if (FALSE) { require(terra) require(dplyr)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Bivariate Dependence Plots: # In different resolutions p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   resolution = 50 )  p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   resolution = 25 )  # With projection and training boundaries p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"elevation\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"rectangle\" )  p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"elevation\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"convexh\" )  # Customize colors and theme p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"convexh\",   color_gradient =     c(       \"#122414\", \"#183C26\", \"#185437\", \"#106D43\", \"#0F874C\",       \"#2D9F54\", \"#61B463\", \"#8DC982\", \"#B3E0A7\", \"#D7F9D0\"     ),   color_training_boundaries = \"purple\",   theme = ggplot2::theme_dark() ) }"},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"Create partial dependence plots explore marginal effect predictors modeled abundance","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"","code":"p_abund_pdp(   model,   predictors = NULL,   resolution = 100,   resid = FALSE,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   projection_data = NULL,   rug = FALSE,   colorl = c(\"#462777\", \"#6DCC57\"),   colorp = \"black\",   alpha = 0.2,   theme = ggplot2::theme_classic(),   sample_size = NULL,   training_raster = NULL,   x_coord = NULL,   y_coord = NULL )"},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"model model object found first element list returned function fit_abund_ tune_abund_ function families predictors character. Vector predictor name(s) calculate partial dependence plots. NULL predictors used. Default NULL resolution numeric. Number equally spaced points predict abundance values continuous predictors. Default 50 resid logical. Calculate residuals based training data. Default FALSE training_data data.frame tibble. Database response predictor values used fit model. Default NULL invert_transform logical. TRUE, inverse transformation response variable applied. response_name character. Name response variable. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL rug logical. Add rug plot partial dependence plot. Default FALSE colorl character. Vector colors plot partial dependence curves. Default c(\"#462777\", \"#6DCC57\") colorp character. Color plot residuals. Default \"black\" alpha numeric. Transparency residuals. Default 0.2 theme ggplot2 theme. Default ggplot2::theme_classic() sample_size vector. CNN . vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) training_raster terra SpatRaster object. CNN . raster containing predictor variables used tune_abund_cnn fit_abund_cnn. x_coord character. CNN . name column containing longitude information observation. y_coord character. CNN . name column containing latitude information observation.","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"ggplot object","code":""},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"function creates partial dependent plots explore marginal effect predictors modeled abundance. projection_data used, function extract minimum maximum values found region time period model projected. range projection data greater training data plotted different color. Partial dependence plot used interpret model explore model may extrapolate outside environmental conditions used train model.","code":""},{"path":[]},{"path":"https://xx.github.io/adm/reference/p_abund_pdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"","code":"if (FALSE) { require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast()  data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Partial Dependence Plots:  # In different resolutions p_abund_pdp(   model = mglm,   resolution = 50,   training_data = some_sp,   response_name = \"Abundance\" )  p_abund_pdp(   model = mglm,   resolution = 5,   training_data = some_sp,   response_name = \"Abundance\" )  # Especific variables and different resulotions p_abund_pdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   response_name = \"Abundance\" )  # With residuals and rug plot p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   resid = TRUE )  p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   rug = TRUE )  p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   resid = TRUE,   rug = TRUE )  # Partial depence plot for training and projection condition found in a projection area p_abund_pdp(   model = mglm,   training_data = some_sp,   projection_data = envar,   response_name = \"Abundance\",   rug = TRUE )  # Custumize colors and theme p_abund_pdp(   model = mglm,   predictors = NULL,   resolution = 100,   resid = TRUE,   training_data = some_sp,   projection_data = envar,   colorl = c(\"blue\", \"red\"),   colorp = \"darkgray\",   alpha = 0.4,   theme = ggplot2::theme_dark() ) }"},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the output resolution of a layer — res_calculate","title":"Calculate the output resolution of a layer — res_calculate","text":"Calculate output resolution layer pooling operation Convolutional Neural Network.","code":""},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the output resolution of a layer — res_calculate","text":"","code":"res_calculate(   type = c(\"layer\", \"pooling\"),   in_res,   kernel_size,   stride,   padding )"},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the output resolution of a layer — res_calculate","text":"type string. Accepted values \"layer\" \"pooling\". in_res integer. represents resolution input layer. kernel_size integer. refers size kernel used convolution pooling operation. stride integer. stride length convolution pooling operation. used type \"layer\". padding integer. amount padding added input layer. used type \"layer\"","code":""},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the output resolution of a layer — res_calculate","text":"function returns integer output resolution.","code":""},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the output resolution of a layer — res_calculate","text":"type \"layer\" output resolution calculated ((in_res - kernel_size + (2 * padding)) / stride) + 1. type \"pooling\", output resolution calculated floor division in_res kernel_size.","code":""},{"path":"https://xx.github.io/adm/reference/res_calculate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the output resolution of a layer — res_calculate","text":"","code":"if (FALSE) {  # Calculating output resolution for a convolution layer res_calculate(type = \"layer\", in_res = 12, kernel_size = 2, stride = 2, padding = 0)  # Calculating output resolution for a pooling layer res_calculate(type = \"pooling\", in_res = 12, kernel_size = 2) }"},{"path":"https://xx.github.io/adm/reference/select_arch_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"Select architectures Convolutional Neural Network Deep Neural Network","code":""},{"path":"https://xx.github.io/adm/reference/select_arch_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"","code":"select_arch_list(   arch_list,   type = c(\"dnn\", \"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )"},{"path":"https://xx.github.io/adm/reference/select_arch_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"arch_list list. Containing Convolutional Neural Network Deep Neural Network architectures. type character. Indicating type network. Options \"dnn\" \"cnn\". method character. Indicating method select architectures. Default \"percentile\". n_samples integer. Specifying number samples select per group. Default 1. min_max logical. TRUE, include networks minimal maximal parameters.","code":""},{"path":"https://xx.github.io/adm/reference/select_arch_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"list : arch_list: list containing torch neural networks arch_dict: list matrices describing structure networks changes: tibble information neural networks name changes, number parameters deepness","code":""},{"path":"https://xx.github.io/adm/reference/select_arch_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"","code":"if (FALSE) { # Generate some big list of architectures combining all argument values big_arch_list <- generate_arch_list(   type = \"dnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = seq(from = 2, to = 6, by = 1),   n_neurons = c(8, 16, 32, 64) )  length(big_arch_list$arch_list) # 5456 architectures!  # It can be reduced sampling network architectures by its parameters number  reduced_arch_list <- big_arch_list %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1, # Keep at least one of each deepness   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(reduced_arch_list$arch_list) # from 5456 to 92 architectures!!  # See architectures names, deepness and number of parameters reduced_arch_list$changes }"},{"path":"https://xx.github.io/adm/reference/sppabund.html","id":null,"dir":"Reference","previous_headings":"","what":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"data set containing species abundance three species, partition folds, environmental variables.","code":""},{"path":"https://xx.github.io/adm/reference/sppabund.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"","code":"sppabund"},{"path":"https://xx.github.io/adm/reference/sppabund.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"tibble 2767 rows 12 variables: species species names ind_ha species abundance expressed individuals per hectare x longitude species occurrences y latitude species occurrences bio1 bioclimatic variable related annual mean temperature bio3 bioclimatic variable related isothermality bio12 bioclimatic variable related annual precipitation bio15 bioclimatic variable related precipitation seasonality cfvo edaphic variable related volumetric fraction coarse fragments elevation topographic elevation sand edaphic variable related soil sand content eoc ecoregion .part1 ... .part3 repeate k-folds","code":""},{"path":"https://xx.github.io/adm/reference/sppabund.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"","code":"if (FALSE) { require(dplyr) data(\"sppabund\") sppabund }"},{"path":"https://xx.github.io/adm/reference/tune_abund_cnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"Fit validate Convolutional Neural Network exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_cnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"","code":"tune_abund_cnn(   data,   response,   predictors,   predictors_f = NULL,   x,   y,   rasters,   sample_size,   partition,   predict_part = FALSE,   grid = NULL,   architectures = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_cnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") x character. Column name longitude data. y character. Column name latitude data. rasters character. Path raster file environmental variables. sample_size numeric. dimension, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"batch_size\", \"n_epochs\", \"learning_rate\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: batch_size = 2^seq(4, 6) n_epochs = 10 learning_rate = seq(= 0.1, = 0.2, = 0.1) validation_patience = 2 fitting_patience = 5. case one hyperparameters provided, function complete grid default values. architectures list character. list object containing list architectures (nn_modules_generators torch), called \"arch_list\", list matrices describing architecture, called (\"arch_dict\"); use generate_arch_list function create . also possible use \"fit_intern\", construct default neural network architecture fit_abund_cnn. NULL, list architectures generated. Default NULL metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_cnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_cnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate some architectures many_archs <- generate_arch_list(   type = \"cnn\",   number_of_features = 3,   number_of_outputs = 1,   n_layers = c(2, 3),   n_neurons = c(16, 32),   sample_size = c(11, 11),   number_of_fc_layers = c(1), # fully connected layers   fc_layers_size = c(16),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   batch_norm = TRUE ) %>% select_arch_list(   type = c(\"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )  # Create a grid # Obs.: the grid is tested with every architecture, thus it can get very large. cnn_grid <- expand.grid(   learning_rate = c(0.01, 0.005),   n_epochs = c(50, 100),   batch_size = c(32),   validation_patience = c(2, 4),   fitting_patience = c(2, 4),   stringsAsFactors = FALSE )  # Tune a cnn model tuned_cnn <- tune_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = cnn_grid,   rasters = system.file(\"external/envar.tif\", package = \"adm\"),   x = \"x\",   y = \"y\",   sample_size = c(11, 11),   architectures = many_archs,   n_cores = 3 ) tuned_cnn  # It is also possible to use a only one architecture one_arch <- generate_cnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = T )  tuned_cnn_2 <- tune_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = cnn_grid,   architectures = one_arch,   rasters = system.file(\"external/envar.tif\", package = \"adm\"),   x = \"x\",   y = \"y\",   sample_size = c(11, 11),   n_cores = 3 )  tuned_cnn_2 }"},{"path":"https://xx.github.io/adm/reference/tune_abund_dnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"Fit validate Deep Neural Network model exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_dnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"","code":"tune_abund_dnn(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   grid = NULL,   architectures = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_dnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"batch_size\", \"n_epochs\", \"learning_rate\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: batch_size = 2^seq(4, 6) n_epochs = 10 learning_rate = seq(= 0.1, = 0.2, = 0.1) validation_patience = 2 fitting_patience = 5. case one hyperparameters provided, function complete grid default values. architectures list character. list object containing list architectures (nn_modules_generators torch), called \"arch_list\", list matrices describing architecture, called (\"arch_dict\"); use generate_arch_list function create . also possible use \"fit_intern\", construct default neural network architecture fit_abund_dnn. NULL, list architectures generated. Default NULL metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_dnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_dnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate some architectures many_archs <- generate_arch_list(   type = \"dnn\",   number_of_features = 3,   number_of_outputs = 1,   n_layers = c(2, 3),   n_neurons = c(6, 12, 16) ) %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )  # Create a grid # Obs.: the grid is tested with every architecture, thus it can get very large. dnn_grid <- expand.grid(   learning_rate = c(0.01, 0.005),   n_epochs = c(50, 100),   batch_size = c(32),   validation_patience = c(2, 4),   fitting_patience = c(2, 4),   stringsAsFactors = FALSE )  # Tune a DNN model tuned_dnn <- tune_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = dnn_grid,   architectures = many_archs,   n_cores = 3 )  tuned_dnn  # It is also possible to use a only one architecture one_arch <- generate_dnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   number_of_hidden_layers = 3,   hidden_layers_size = c(8, 16, 8),   batch_norm = TRUE )  tuned_dnn_2 <- tune_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = dnn_grid,   architectures = one_arch,   n_cores = 3 )  tuned_dnn_2 }"},{"path":"https://xx.github.io/adm/reference/tune_abund_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"Fit validate Generalized Additive Models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"","code":"tune_abund_gam(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe 'distribution', 'inter' columns values combinations rows. grid provided, function create default grid combining next hyperparameters: distribution = families selected family_selector, inter = \"automatic\". case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\") n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"list object : model: \"gamlss\" object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"","code":"if (FALSE) { require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\") # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3) # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist() # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2) # Explore different family distributions suitable_distributions <- family_selector(data = some_sp, response = \"ind_ha\") suitable_distributions # Create a grid gam_grid <- expand.grid(   inter = \"automatic\",   distribution = suitable_distributions$family_call,   stringsAsFactors = FALSE ) # Tune a GAM model tuned_gam <- tune_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   fit_formula = formula(\"ind_ha ~ bio12 + elevation + sand + eco\"),   sigma_formula = formula(\"ind_ha ~ bio12 + elevation\"),   nu_formula = formula(\"ind_ha ~ bio12 + elevation\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = gam_grid,   n_cores = 3 )  tuned_gam }"},{"path":"https://xx.github.io/adm/reference/tune_abund_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"Fit validate Generalized Boosted Regression models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"","code":"tune_abund_gbm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   distribution,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_gbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"n.trees\", \"interaction.depth\", \"n.minobsinnode\" \"shrinkage\" columns values rows. grid provided, function create default grid combining next hyperparameters: n.trees = c(100, 200, 300), interaction.depth = c(1, 2, 3), n.minobsinnode = c(5, 10, 15), shrinkage = seq(0.001, 0.1, = 0.05). case one hyperparameters provided, function complete grid default values. distribution character. string specifying distribution used. See gbm::gbm documentation details. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"list object : model: \"gbm\" object gbm package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid gbm_grid <- expand.grid(   interaction.depth = c(2, 4, 8, 16),   n.trees = c(100, 500, 1000),   n.minobsinnode = c(2, 5, 8),   shrinkage = c(0.1, 0.5, 0.7),   stringsAsFactors = FALSE )  tuned_gbm <- tune_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = gbm_grid,   distribution = \"gaussian\",   n_cores = 3 )  tuned_gbm }"},{"path":"https://xx.github.io/adm/reference/tune_abund_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"Fit validate Generalized Linear Models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"","code":"tune_abund_glm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"distribution\", \"poly\", \"inter_order\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: poly = c(1, 2, 3), inter_order = c(0, 1, 2), distribution = families_hp$family_call. case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"list object : model: \"gamlss\" object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"","code":"if (FALSE) { require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions suitable_distributions <- family_selector(data = some_sp, response = \"ind_ha\") suitable_distributions  # Create a grid glm_grid <- expand.grid(   poly = c(2, 3),   inter_order = c(1, 2),   distribution = suitable_distributions$family_call,   stringsAsFactors = FALSE )  # Tune a GLM model tuned_glm <- tune_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   fit_formula = formula(\"ind_ha ~ bio12 + elevation + sand + eco\"),   sigma_formula = formula(\"ind_ha ~ bio12 + elevation\"),   nu_formula = formula(\"ind_ha ~ bio12 + elevation\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = glm_grid,   n_cores = 3 )  tuned_glm }"},{"path":"https://xx.github.io/adm/reference/tune_abund_net.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"Fit validate Shallow Neural Networks models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_net.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"","code":"tune_abund_net(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_net.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"size\" \"decay\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: size = seq(= length(c(predictors, predictors_f)), = 50, = 2), decay = seq(= 0, = 0.9, = 0.1). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_net.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"list object : model: \"nnet\" class object nnet package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_net.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid net_grid <- expand.grid(   size = seq(from = 8, to = 32, by = 6),   decay = seq(from = 0, to = 0.4, by = 0.01),   stringsAsFactors = FALSE )  # Tune a NET model tuned_net <- tune_abund_net(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = net_grid,   n_cores = 3 )  tuned_net }"},{"path":"https://xx.github.io/adm/reference/tune_abund_raf.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"Fit validate Random Forest models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_raf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"","code":"tune_abund_raf(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_raf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"mtry\" \"ntree\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: mtry = seq(2, length(predictors), = 1), ntree = seq(500, 1000, = 100). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_raf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"list object : model: \"randomForest\" class object randomForest package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_raf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species two\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid raf_grid <- expand.grid(   mtry = seq(from = 2, to = 3, by = 1),   ntree = seq(from = 500, to = 1000, by = 100),   stringsAsFactors = FALSE )  # Tune a RAF model tuned_raf <- tune_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = raf_grid,   n_cores = 3 )  tuned_raf }"},{"path":"https://xx.github.io/adm/reference/tune_abund_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"Fit validate Support Vector Machine models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"","code":"tune_abund_svm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. grid tibble data.frame. dataframe \"kernel\", \"sigma\", \"C\" columns values combinations rows. now grid provided, funcion create default grid combining next hyperparameters: C = seq(0.2, 1, = 0.2), sigma = \"automatic\", kernel = c(\"rbfdot\", \"laplacedot\"). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_svm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_svm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"","code":"if (FALSE) { require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid svm_grid <- expand.grid(   sigma = \"automatic\",   C = c(0.5, 1, 2),   kernel = c(\"rbfdot\", \"laplacedot\"),   stringsAsFactors = FALSE )  # Tune a SVM model tuned_svm <- tune_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = svm_grid,   n_cores = 3 )  tuned_svm }"},{"path":"https://xx.github.io/adm/reference/tune_abund_xgb.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"Fit validate Extreme Gradient Boosting models exploration hyper-parameters optimize performance","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_xgb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"","code":"tune_abund_xgb(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   grid = NULL,   objective = \"reg:squarederror\",   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"https://xx.github.io/adm/reference/tune_abund_xgb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"n.trees\", \"interaction.depth\", \"n.minobsinnode\" \"shrinkage\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: nrounds = c(100, 200, 300), max_depth = c(4, 6, 8), eta = c(0.2, 0.4, 0.5), gamma = c(1, 5, 10), colsample_bytree = c(0.5, 1, 2), min_child_weight = c(0.5, 1, 2), subsample = c(0.5, 0.75, 1). case one hyperparameters provided, function complete grid default values. objective character. learning task corresponding learning objective. Default \"reg:squarederror\", regression squared loss. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_xgb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"list object : model: \"xgb.Booster\" object xgboost package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"https://xx.github.io/adm/reference/tune_abund_xgb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"","code":"if (FALSE) { require(dplyr) # Database with species abundance and x and y coordinates data(\"sppabund\") # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species two\") %>%   dplyr::select(-.part2, -.part3) # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist() # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2) # Create a grid xgb_grid <- expand.grid(   nrounds = c(100, 300),   max_depth = c(4, 6, 8),   eta = c(0.2, 0.5),   gamma = c(1, 5, 10),   colsample_bytree = c(0.5, 1),   min_child_weight = c(0.5, 1, 2),   subsample = c(0.5, 1),   stringsAsFactors = FALSE ) # Tune a XGB model tuned_xgb <- tune_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = xgb_grid,   objective = \"reg:squarederror\",   n_cores = 3 ) tuned_xgb }"}]
