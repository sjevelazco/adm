[{"path":"/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"x x. Author. x x. Author, maintainer.","code":""},{"path":"/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"x x, x x (2024). adm: Tools Constructing, Predicting, Post-Processing Abundance-based Distribution Models. R package version 0.0.2.","code":"@Manual{,   title = {adm: Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models},   author = {x x and x x},   year = {2024},   note = {R package version 0.0.2}, }"},{"path":[]},{"path":"/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"package aims support construction Abundance-based distribution models, including data preparation, model fitting, prediction, model exploration. package offers several modeling approaches (.e., algorithms), can fine-tuned customized users. Models can predicted geographic space explored terms performance response curves. modeling workflows adm constructed based combination distinct functions simple outputs, adm can easily integrated packages.","code":""},{"path":"/index.html","id":"structure-of-adm","dir":"","previous_headings":"","what":"Structure of adm","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"adm functions grouped three categories: modeling, post-modeling, miscellaneus tools","code":""},{"path":"/index.html","id":"i-modeling","dir":"","previous_headings":"Structure of adm","what":"i) modeling","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Functions tune, fit validate models nine different algorithms, suite possible model-specific hyper-parametera Fit validate models without hyper-parameters tuning fit_abund_cnn(): Fit validate Convolutional Neural Network Model fit_abund_dnn(): Fit validate Deep Neural Network model fit_abund_gam(): Fit validate Generalized Additive Models fit_abund_gbm(): Fit validate Generalized Boosted Regression models fit_abund_glm(): Fit validate Generalized Linear Models fit_abund_net(): Fit validate Artificial Neural Network models fit_abund_raf(): Fit validate Random Forests models fit_abund_svm(): Fit validate Support Vector Machine models fit_abund_xgb(): Fit validate Extreme Gradient Boosting models Fit validate models hyper-parameters tuning tune_abund_cnn(): Fit validate Convolutional Neural Network exploration hyper-parameters optimize performance tune_abund_dnn(): Fit validate Deep Neural Network model exploration hyper-parameters optimize performance tune_abund_gam(): Fit validate Generalized Additive Models exploration hyper-parameters optimize performance tune_abund_gbm(): Fit validate Generalized Boosted Regression models exploration hyper-parameters optimize performance tune_abund_glm(): Fit validate Generalized Linear Models exploration hyper-parameters optimize performance tune_abund_net(): Fit validate Shallow Neural Networks models exploration hyper-parameters optimize performance tune_abund_raf(): Fit validate Random Forest models exploration hyper-parameters optimize performance tune_abund_svm(): Fit validate Support Vector Machine models exploration hyper-parameters optimize performance tune_abund_xgb(): Fit validate Extreme Gradient Boosting models exploration hyper-parameters optimize performance Modeling evalution adm_eval(): Calculate different model performance metrics","code":""},{"path":"/index.html","id":"ii-post-modeling","dir":"","previous_headings":"Structure of adm","what":"ii) post-modeling","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Functions predict abundance across space construct partial dependence plots explore relationships abundance environmental predictors adm_predict(): Spatial predictions individual ensemble models p_abund_bpdp(): Bivariate partial dependence plots abundance-based distribution models p_abund_pdp(): Partial dependent plots abundance-based distribution models data_abund_bpdp(): Calculate data construct bivariate partial dependence plots data_abund_pdp(): Calculate data construct partial dependence plots","code":""},{"path":"/index.html","id":"iii-miscellaneous-tools","dir":"","previous_headings":"Structure of adm","what":"iii) miscellaneous tools","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"Extra functions support modeling workflow, including data handling, transformations, hyper-parameters selection. adm_extract(): Extract values spatial raster based x y coordinates adm_summarize(): Merge model performance tables adm_transform(): Performs data transformation variable based specified method. balance_dataset(): Balance database given absence-presence ratio cnn_make_samples(): Creates sample data Convolutional Neural Network croppin_hood(): Crop rasters around point (Convolutional Neural Networks) family_selector(): Select probability distributions GAM GLM generate_arch_list(): Generate architecture list Deep Neural Network Convolutional Neural Network generate_cnn_architecture(): Generate architectures Convolutional Neural Network generate_dnn_architecture(): Generate architectures Deep Neural Network model_selection(): Best hyper-parameters selection res_calculate(): Calculate output resolution layer select_arch_list(): Select architectures Convolutional Neural Network Deep Neural Network","code":""},{"path":"/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools for Constructing, Predicting, and Post-Processing Abundance-based Distribution Models","text":"can install development version adm github","code":"# For Windows and Mac OS operating systems remotes::install_github(\"x/adm\")"},{"path":"/reference/adm_eval.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate different model performance metrics — adm_eval","title":"Calculate different model performance metrics — adm_eval","text":"function, supplied observed predicted values, calculates  accuracy, discrimination, precision two returns values tibble table. accuracy evaluated mean absolute error. discrimination calculated using Spearman correlation, Pearson correlation, intercept slope linear regression observed predicted values. precision obtained standard deviations predicted observed values.","code":""},{"path":"/reference/adm_eval.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate different model performance metrics — adm_eval","text":"","code":"adm_eval(obs, pred)"},{"path":"/reference/adm_eval.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate different model performance metrics — adm_eval","text":"obs numeric. Observed abundance pred numeric. Predicted abundance","code":""},{"path":"/reference/adm_eval.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate different model performance metrics — adm_eval","text":"tibble next columns: corr_spear, corr_pear, mae, inter, slope, pdisp(see details)","code":""},{"path":"/reference/adm_eval.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate different model performance metrics — adm_eval","text":"function calculate metric related accuracy, discrimination, precision model: Accuracy: mean absolute error (mae) Discrimination: Spearman<U+2019>s rank correlation (corr_spear) Discrimination: Pearson<U+2019>s correlation (corr_pear) Discrimination: regression intercept observed predicted values (inter) Discrimination: regression slope observed predicted values (slope) Precision: ratio predicted observed standard deviation (pdisp) details see Waldock et al. (2022)","code":""},{"path":"/reference/adm_eval.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate different model performance metrics — adm_eval","text":"Waldock, C., Stuart-Smith, R.D., Albouy, C., Cheung, W.W.L., Edgar, G.J., Mouillot, D., Tjiputra, J., Pellissier, L., 2022. quantitative review abundance-based species distribution models. Ecography https://doi.org/10.1111/ecog.05694","code":""},{"path":"/reference/adm_eval.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate different model performance metrics — adm_eval","text":"","code":"if (FALSE) { # \\dontrun{ pred_a <- c(   3, 2, 0, 0, 2, 5, 1, 3, 1, 2, 1, 1, 2, 5, 4,   1, 2, 5, 3, 3, 4, 3, 2, 0, 2, 1, 2, 2, 1, 4,   4, 2, 2, 1, 6, 1, 1, 3, 5, 0, 1, 1, 0, 1, 2 ) obs_a <- c(   3, 1, 1, 3, 2, 3, 0, 3, 5, 3, 4, 2, 0, 5, 2,   1, 2, 2, 3, 6, 3, 2, 4, 2, 1, 2, 3, 5, 0, 3,   3, 2, 1, 2, 3, 2, 2, 1, 2, 3, 3, 1, 2, 1, 4 )  adm_eval(obs = obs_a, pred = pred_a) } # }"},{"path":"/reference/adm_extract.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract values from a spatial raster based on x and y coordinates — adm_extract","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"function extracts environmental data given x y coordinates","code":""},{"path":"/reference/adm_extract.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"","code":"adm_extract(data, x, y, env_layer, variables = NULL, filter_na = TRUE)"},{"path":"/reference/adm_extract.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"data data.frame tibble. Database species abundance x y coordinates x character. Column name spatial x coordinates y character. Column name spatial y coordinates env_layer SpatRaster. Raster environmental variables. variables character. Vector variable names predictor (environmental) variables Usage variables = c(\"elevation\", \"sand\", \"cfvo\"). variable specified, function return data layers. Default NULL filter_na logical. filter_na = TRUE (default), rows NA values environmental variables removed returned tibble.","code":""},{"path":"/reference/adm_extract.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"tibble returns original data base additional columns extracted environmental variables x y location SpatRaster object used 'env_layer'","code":""},{"path":"/reference/adm_extract.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract values from a spatial raster based on x and y coordinates — adm_extract","text":"","code":"if (FALSE) { # \\dontrun{ require(terra)  # Datasbase with species abundance and x and y coordinates data(\"sppabund\")  # Raster data with environmental variables envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Extract data for a single species some_sp <- sppabund %>%   filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  # Extract environmental data from envar raster for all locations in spp ex_spp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar,     variables = NULL,     filter_na = FALSE   )  # Extract environmental for two variables and remove rows with NAs ex_spp2 <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar,     variables = c(\"bio1\", \"elevation\"),     filter_na = TRUE   )  ex_spp ex_spp2 } # }"},{"path":"/reference/adm_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Spatial predictions from individual and ensemble models — adm_predict","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"function allows geographical prediction one models constructed fit_ tune_ function set, models fitted esm_ function set (.e., ensemble small models approach), models constructed fit_ensemble function. can return continuous continuous binary predictions one thresholds","code":""},{"path":"/reference/adm_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"","code":"adm_predict(   models,   pred,   training_data = NULL,   nchunk = 1,   predict_area = NULL,   invert_transform = NULL,   transform_negative = FALSE,   sample_size = c(11, 11) )"},{"path":"/reference/adm_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"models list one models fitted fit_ tune_ functions. case use models fitted fit_ensemble esm_ family function one model used. Usage models = mglm models = list(mglm, mraf, mgbm) pred SpatRaster. Raster layer predictor variables. Names layers must exactly match used model fitting. training_data data.frame tibble. Data used fit models. necessary predict GAM GLM models. Default NULL nchunk integer. Number chunks split data used predict models (.e., SpatRaster used pred argument). Predicting models chunks helps reduce memory requirements cases models predicted large scales high resolution. Default = 1 predict_area SpatVector, SpatialPolygon, SpatialPolygonDataFrame. Spatial polygon used restring prediction given region. Default = NULL invert_transform logical. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Default NULL transform_negative logical. TRUE, negative values prediction set zero. default FALSE. sample_size numeric. vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11)","code":""},{"path":"/reference/adm_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"list SpatRaster continuous /binary predictions","code":""},{"path":"/reference/adm_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Spatial predictions from individual and ensemble models — adm_predict","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  data(\"sppabund\") envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Extract data some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  some_sp  some_sp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar   )  # Partition some_sp <- flexsdm::part_random(   data = some_sp,   pr_ab = \"ind_ha\",   method = c(method = \"rep_kfold\", folds = 3, replicates = 3) )   ## %######################################################%## #                                                          # ####          Create different type of models           #### #                                                          # ## %######################################################%## # Fit some models # require(gamlss) # m1 <- gamlss::fitDist(some_sp$ind_ha, type=\"realline\") # m1$fits # m1$failed # # m1 <- gamlss(ind_ha ~ pb(elevation) + pb(sand) + pb(bio3) + pb(bio12), family=NO, data=some_sp) # choosen_dist <- gamlss::chooseDist(m1, parallel=\"snow\", ncpus=4, type=\"realAll\")  mgam <- fit_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   predictors_f = \"eco\",   partition = \".part\",   distribution = gamlss.dist::NO() )  mraf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\", )  mgbm <- fit_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\",   distribution =   )   ## %######################################################%## #                                                          # ####            ' ####      Predict models              #### #                                                          # ## %######################################################%##  # adm_predict can be used for predict one or more models fitted with fit_ or tune_ functions  # a single model ind_p <- sdm_predict(   models = mglm,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # a list of models list_p <- sdm_predict(   models = list(mglm, mraf, mgbm),   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # Predict an ensemble model # (only is possilbe use one fit_ensemble) ensemble_p <- sdm_predict(   models = mensemble,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  # Predict an ensemble of small models # (only is possible to use one ensemble of small models) small_p <- sdm_predict(   models = msmall,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL )  ## %######################################################%## #                                                          # ####              Predict model using chunks            #### #                                                          # ## %######################################################%## # Predicting models in chunks helps reduce memory requirements in # cases where models are predicted for large scales and high resolution  ind_p <- sdm_predict(   models = mglm,   pred = somevar,   thr = \"max_fpb\",   con_thr = FALSE,   predict_area = NULL,   nchunk = 4 ) } # }"},{"path":"/reference/adm_summarize.html","id":null,"dir":"Reference","previous_headings":"","what":"Merge model performance tables — adm_summarize","title":"Merge model performance tables — adm_summarize","text":"Merge model performance tables","code":""},{"path":"/reference/adm_summarize.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Merge model performance tables — adm_summarize","text":"","code":"adm_summarize(models)"},{"path":"/reference/adm_summarize.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Merge model performance tables — adm_summarize","text":"models list. list single several models fitted fit_ tune_ functions. Usage models = list(mod1, mod2, mod3)","code":""},{"path":"/reference/adm_summarize.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Merge model performance tables — adm_summarize","text":"tibble object combined model performance input models. Models fit tune include model performance best hyperparameters.","code":""},{"path":"/reference/adm_summarize.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Merge model performance tables — adm_summarize","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  data(\"sppabund\") envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Species abundance data, coordinates, and partition some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y, .part) some_sp  # Extract data some_sp <-   adm_extract(     data = some_sp,     x = \"x\",     y = \"y\",     env_layer = envar   )  # Fit RAF m_raf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\", )  # Fit SVM m_svm <- fit_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\" )  # XGB m_xbg <- fit_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   partition = \".part\" )   perf <- adm_summarize(list(m_svm, m_raf, m_xbg))  perf } # }"},{"path":"/reference/adm_transform.html","id":null,"dir":"Reference","previous_headings":"","what":"Performs data transformation on a variable based on the specified method. — adm_transform","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"function transforms data tibble SpatRaster object method specified available methods \"01\", \"zscore\", \"log\", \"round\".","code":""},{"path":"/reference/adm_transform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"","code":"adm_transform(data, variable, method, inverse = FALSE, t_terms = NULL)"},{"path":"/reference/adm_transform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"data data.frame, tibble, SpatRaster containing data. variable character string specifying variable (column) transformed. method character string specifying method used transformation. Available methods \"01\", \"zscore\", \"log\", \"round.\" \"01\", scales variable 0 1 using formula (x - min(x)) (max(x) - min(x)). \"zscore\", standardizes variable subtracting mean dividing standard deviation. \"log\", applies natural logarithm transformation variable. \"log1\", sums 1 applies natural logarithm transformation variable. \"round\", rounds variable's values nearest whole numbers. inverse logical. Invert transformation? t_terms vector. c(,b): \"01\", = min(x), b = max(x). \"zscore\", = mean(x), b = sd(x). \"log\" \"log1, needed. invert \"round\" transformations.","code":""},{"path":"/reference/adm_transform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"data.frame tibble transformed variable added new column. new column's name original variable name followed underscore method name.","code":""},{"path":"/reference/adm_transform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performs data transformation on a variable based on the specified method. — adm_transform","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  data(\"sppabund\") # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(species, ind_ha, x, y)  # Transform abundance data to 0-1 some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"01\" ) some_sp_2  # Transform abundance data z-score some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"zscore\" ) some_sp_2  # Transform abundance data log some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"log\" ) some_sp_2  # Transform abundance data log some_sp_2 <- adm_transform(   data = some_sp,   variable = \"ind_ha\",   method = \"round\" ) some_sp_2  # TODO Invert transformation  # TODO Transform raster data } # }"},{"path":"/reference/balance_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Balance database at a given absence-presence ratio — balance_dataset","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"function balances given database based specified ratio absence presence. randomly removes excess absence database achieve specified ratio. function interprets absence data abundance equal zero.","code":""},{"path":"/reference/balance_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"","code":"balance_dataset(data, response, absence_ratio)"},{"path":"/reference/balance_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"data data.frame tibble. Database contains columns abundance. response string. name column `data` representing response variable. Note absence interpreted data abundance equal zero. Usage response = \"ind_ha\" absence_ratio numeric. desired ratio presence absence response column. E.g., set 1 function remove absence number presence. set 1.5, function remove absence 1.5 times number presence. Usage absence_ratio = 0.5","code":""},{"path":"/reference/balance_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"Returns balanced data.frame tibble absence-presence ratio response column equal absence_ratio","code":""},{"path":"/reference/balance_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Balance database at a given absence-presence ratio — balance_dataset","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  data(\"sppabund\") some_sp <- sppabund %>%   dplyr::filter(species == \"Species three\") %>%   dplyr::select(species, ind_ha, x, y)  table(some_sp$ind_ha > 0) # Note that the dataset is almost balanced # However, as an example, let's assume that we want to reduce # the number of absences half of the number of presences  some_sp_2 <- balance_dataset(   data = some_sp,   response = \"ind_ha\",   absence_ratio = 0.5 )  table(some_sp$ind_ha > 0) table(some_sp_2$ind_ha > 0) } # }"},{"path":"/reference/cnn_make_samples.html","id":null,"dir":"Reference","previous_headings":"","what":"Creates sample data for Convolutional Neural Network — cnn_make_samples","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"function creates array input images associated responses can utilized train Convolutional Neural Network.","code":""},{"path":"/reference/cnn_make_samples.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"","code":"cnn_make_samples(   data,   x,   y,   response,   raster,   raster_padding = FALSE,   padding_method = NULL,   size = 5 )"},{"path":"/reference/cnn_make_samples.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"data data.frame tibble. Database includes longitude, latitude, response columns. x string. Specifying name column longitude data. y string. Specifying name column latitude data. response string. Specifying name column response. raster SpatRaster. Raster predictor data cropped. raster_padding logical. TRUE, raster padded cropping extends beyond boundaries. Useful ensuring focal cells size output even edges raster. Default FALSE padding_method string NULL. Method used padding raster raster_padding TRUE. Options \"mean\", \"median\", \"zero\". Ignored raster_padding FALSE. Default NULL size numeric. Size cropped raster, number o cell direction focal cell","code":""},{"path":"/reference/cnn_make_samples.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"list two elements - 'predict' (list input images) 'response' (response values). element 'predictors' list array representing cropped image input raster.","code":""},{"path":"/reference/cnn_make_samples.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Creates sample data for Convolutional Neural Network — cnn_make_samples","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  cnn_samples <- cnn_make_samples(   data = some_sp,   x = \"x\", # x coordinates for each point   y = \"y\", # y coordinates for each point   response = \"ind_ha\",   raster = envar[[c(\"bio12\", \"sand\", \"elevation\")]],   size = 5 # how many pixels from point to border? )  length(cnn_samples$predictors) #  938 matrix sets dim(cnn_samples$predictors[[1]]) # three 11x11 channels cnn_samples$predictors[[1]] # representing predictor variables rast(cnn_samples$predictors[[1]]) %>% plot()  cnn_samples$response[[1]] # linked to a label } # }"},{"path":"/reference/croppin_hood.html","id":null,"dir":"Reference","previous_headings":"","what":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"Crop rasters single spatial point. Function used internally construct Convolutional Neural Networks","code":""},{"path":"/reference/croppin_hood.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"","code":"croppin_hood(   occ,   x,   y,   raster,   size,   raster_padding = FALSE,   padding_method = NULL )"},{"path":"/reference/croppin_hood.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"occ tibble data.frame. Database response, predictors, partition values x character. Column name spatial x coordinates y character. Column name spatial y coordinates raster SpatRaster. Raster environmental variables. size numeric. Size cropped raster, number o cell direction focal cell raster_padding logical. TRUE, raster padded cropping extends beyond boundaries. Useful ensuring focal cells size output even edges raster. Default FALSE padding_method string NULL. Method used padding raster raster_padding TRUE. Options \"mean\", \"median\", \"zero\". Ignored raster_padding FALSE. Default NULL","code":""},{"path":"/reference/croppin_hood.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"SpatRaster. Croped raster","code":""},{"path":"/reference/croppin_hood.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Crop rasters around a point (Convolutional Neural Networks) — croppin_hood","text":"","code":"if (FALSE) { # \\dontrun{ require(terra)  # Datasbase with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   filter(species == \"Species three\")  # Raster data with environmental variables envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # sampl_r <- croppin_hood(occ = some_sp[1, ], x = \"x\", y = \"y\", raster = envar, size = 5) plot(sampl_r) plot(sampl_r[[1]]) points(some_sp[1, c(\"x\", \"y\")], pch = 19) } # }"},{"path":"/reference/data_abund_bpdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"Calculate data construct bivariate partial dependence two predictor set","code":""},{"path":"/reference/data_abund_bpdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"","code":"data_abund_bpdp(   model,   predictors,   resolution = 50,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   training_boundaries = NULL,   projection_data = NULL )"},{"path":"/reference/data_abund_bpdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"model object returned fit_abund tune_abund family functions predictors character. Vector two predictor name(s) plot. NULL predictors plotted. Default NULL resolution numeric. Number equally spaced points predict continuous predictors. Default 50 training_data data.frame tibble. Database response (0,1) predictor values used fit model. Default NULL invert_transform logical. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Default NULL response_name character. Name response variable. Default NULL training_boundaries character. Plot training conditions boundaries based training data (.e., presences, presences absences, etc). training_boundaries = \"convexh\", function delimit training environmental region based convex-hull. training_boundaries = \"rectangle\", function delimit training environmental region based four straight lines. used methods necessary provide data training_data argument. NULL predictors used. Default NULL. projection_data SpatRaster. Raster layer environmental variables used model projection. Default NULL","code":""},{"path":"/reference/data_abund_bpdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"list two tibbles \"pdpdata\" \"resid\". pspdata: data construct partial dependence surface plot, first two column includes values selected environmental variables, third column predicted suitability. training_boundaries: data plot boundaries training data.","code":""},{"path":[]},{"path":"/reference/data_abund_bpdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate data to construct bivariate partial dependence plots — data_abund_bpdp","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Prepare data for Bivariate Partial Dependence Plots bpdp_data <- data_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   resolution = 25,   training_data = some_sp,   response_name = \"Abundance\",   projection_data = envar,   training_boundaries = \"convexh\" )  bpdp_data } # }"},{"path":"/reference/data_abund_pdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate data to construct partial dependence plots — data_abund_pdp","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"Calculate data construct partial dependence plots","code":""},{"path":"/reference/data_abund_pdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"","code":"data_abund_pdp(   model,   predictors,   resolution = 50,   resid = FALSE,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   projection_data = NULL )"},{"path":"/reference/data_abund_pdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"model object returned fit_abund tune_abund family functions predictors character. Vector two predictor name(s) plot. NULL predictors plotted. Default NULL resolution numeric. Number equally spaced points predict continuous predictors. Default 50 resid logical. Calculate residuals based training data. Default FALSE training_data data.frame. Database response predictor values used fit model. Default NULL. Required GLM, GAM, DNN, NET, RAF, SVM models invert_transform vector. vector containing method terms invert transformation response variable. Useful cases response variable transformed one method adm_transform. Usage: \"01\": invert_transform = c(method = \"01\", = min(x), b = max(x)) \"zscore\": invert_transform = c(method = \"zscore\", = mean(x), b = sd(x)) \"log\" \"log1: needed. invert \"round\" transformations. Default NULL response_name character. Name response variable. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL","code":""},{"path":"/reference/data_abund_pdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"list two tibbles \"pdpdata\" \"resid\". pdpdata: data construct partial dependence plots, first column includes values selected environmental variable, second column predicted suitability, third  column range type, two values Training Projecting, referring suitability  calculated within outside range training conditions. Third column returned  \"projection_data\" argument used resid: data plot residuals. first column includes values selected environmental  variable second column predicted suitability.","code":""},{"path":[]},{"path":"/reference/data_abund_pdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate data to construct partial dependence plots — data_abund_pdp","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  # Load  data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast()  data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Prepare data for Partial Dependence Plots pdp_data <- data_abund_pdp(   model = mglm,   predictors = \"bio12\",   resolution = 25,   resid = TRUE,   training_data = some_sp,   response_name = \"Abundance\",   projection_data = envar )  pdp_data } # }"},{"path":"/reference/family_selector.html","id":null,"dir":"Reference","previous_headings":"","what":"Select probability distributions for GAM and GLM — family_selector","title":"Select probability distributions for GAM and GLM — family_selector","text":"Select probability distribution available gamlss.dist suited given response variables (e.g., count, zero-inflated) used fit GAM GLM models. See gamlss.family details.","code":""},{"path":"/reference/family_selector.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select probability distributions for GAM and GLM — family_selector","text":"","code":"family_selector(data, response)"},{"path":"/reference/family_selector.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select probability distributions for GAM and GLM — family_selector","text":"data data.frame tibble. Database species abundance response character. Column name species abundance","code":""},{"path":"/reference/family_selector.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select probability distributions for GAM and GLM — family_selector","text":"tibble family_name, family_call, range, discrete columns (family distribution discrete)","code":""},{"path":"/reference/family_selector.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select probability distributions for GAM and GLM — family_selector","text":"","code":"if (FALSE) { # \\dontrun{ data(sppabund)  family_selector(data = sppabund, response = \"ind_ha\") } # }"},{"path":"/reference/fit_abund_cnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"function used fit convolutional neural network (CNN) model abundance.","code":""},{"path":"/reference/fit_abund_cnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"","code":"fit_abund_cnn(   data,   response,   predictors,   predictors_f = NULL,   x,   y,   rasters,   sample_size,   partition,   predict_part = FALSE,   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = NULL,   verbose = TRUE )"},{"path":"/reference/fit_abund_cnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") x character. name column containing longitude information observation. y character. name column containing latitude information observation. rasters terra SpatRaster object. raster containing predictor variables cropped around observation. sample_size numeric. vector containing dimensions, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE learning_rate numeric. size step taken optimization process. Default = 0.01 n_epochs numeric. Maximum number times learning algorithm work training set. Default = 10 batch_size numeric. batch subset training set used single iteration training process. size batch referred batch size. Default = 32 validation_patience numerical. integer indicating number epochs without loss improvement tolerated algorithm validation process. patience limit exceeded, training ends. Default 2 fitting_patience numerical. validation_patience, final model fitting process. Default 5 custom_architecture Torch nn_module_generator object. neural network architecture used instead internal default one. Default NULL verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_cnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_cnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Convolutional Neural Network Model — fit_abund_cnn","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  envar <- system.file(\"external/envar.tif\", package = \"adm\") envar <- terra::rast(envar)  # Generate an architecture cnn_arch <- generate_cnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = T )  # Fit a CNN model mcnn <- fit_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   x = \"x\",   y = \"y\",   rasters = envar,   sample_size = c(11, 11),   learning_rate = 0.01,   n_epochs = 100,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = cnn_arch,   verbose = TRUE,   predict_part = TRUE )  mcnn } # }"},{"path":"/reference/fit_abund_dnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Deep Neural Network model — fit_abund_dnn","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"Fit validate Deep Neural Network model","code":""},{"path":"/reference/fit_abund_dnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"","code":"fit_abund_dnn(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = NULL,   verbose = TRUE )"},{"path":"/reference/fit_abund_dnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE learning_rate numeric. size step taken optimization process. Default = 0.01 n_epochs numeric. Max number times learning algorithm work training set. Default = 10 batch_size numeric. batch subset training set used single iteration training process. size batch referred batch size. Default = 32 validation_patience numerical. integer indicating number epochs without loss improvement tolerated algorithm validation process. patience limit exceeded, training ends. Default 2 fitting_patience numerical. validation_patience, final model fitting process. Default 5 custom_architecture Torch nn_module_generator object generate_dnn_architecture output. neural network architecture used instead internal default one. Default NULL verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_dnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_dnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Deep Neural Network model — fit_abund_dnn","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate a architecture dnn_arch <- generate_dnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   number_of_hidden_layers = 3,   hidden_layers_size = c(8, 16, 8),   batch_norm = TRUE )  # Fit a NET model mdnn <- fit_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   learning_rate = 0.01,   n_epochs = 10,   batch_size = 32,   validation_patience = 2,   fitting_patience = 5,   custom_architecture = dnn_arch,   verbose = TRUE,   predict_part = TRUE )  mdnn } # }"},{"path":"/reference/fit_abund_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Additive Models — fit_abund_gam","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"Fit validate Generalized Additive Models","code":""},{"path":"/reference/fit_abund_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"","code":"fit_abund_gam(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   distribution = NULL,   inter = \"automatic\",   verbose = TRUE,   control_gamlss = gamlss::gamlss.control(trace = FALSE) )"},{"path":"/reference/fit_abund_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE distribution character. string specifying distribution used. See gamlss.family documentation details. Use distribution = gamlss.dist::(). Default NULL inter integer. Number knots x-axis. Default \"automatic\" verbose logical. FALSE, disables console messages. Default TRUE control_gamlss function. control parameters outer iterations algorithm gamlss See gamlss.control documentation details. Default gamlss.control()","code":""},{"path":"/reference/fit_abund_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"list object : model: \"gamlss\" class object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Additive Models — fit_abund_gam","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions family_selector(data = some_sp, response = \"ind_ha\") %>% tail()  # Fit a GAM model mgam <- fit_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"elevation\", \"sand\", \"bio3\", \"bio12\"),   sigma_formula = ~ elevation + bio3 + bio12,   predictors_f = NULL,   partition = \".part\",   distribution = gamlss.dist::ZAGA() )  mgam } # }"},{"path":"/reference/fit_abund_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"Fit validate Generalized Boosted Regression models","code":""},{"path":"/reference/fit_abund_gbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"","code":"fit_abund_gbm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   distribution,   n.trees = 100,   interaction.depth = 5,   n.minobsinnode = 5,   shrinkage = 0.1,   verbose = TRUE )"},{"path":"/reference/fit_abund_gbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE distribution character. string specifying distribution used. See gbm::gbm documentation details. n.trees integer. total number trees fit. interaction.depth integer. maximum depth tree. Default 5 n.minobsinnode integer. minimum number observations terminal nodes trees. Default 5 shrinkage numeric. learning rate algorithm. Default 0.1 verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_gbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"list object : model: \"gbm\" class object gbm package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Boosted Regression models — fit_abund_gbm","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a GBM model mgbm <- fit_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"gaussian\",   n.trees = 100,   interaction.depth = 5,   n.minobsinnode = 5,   shrinkage = 0.1,   predict_part = TRUE )  mgbm } # }"},{"path":"/reference/fit_abund_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Linear Models — fit_abund_glm","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"Fit validate Generalized Linear Models","code":""},{"path":"/reference/fit_abund_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"","code":"fit_abund_glm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   distribution = NULL,   poly = 0,   inter_order = 0,   control_gamlss = gamlss::gamlss.control(trace = FALSE),   verbose = TRUE )"},{"path":"/reference/fit_abund_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. distribution character. string specifying distribution used. See gamlss.family documentation details. Use distribution = gamlss.dist::(). Default NULL poly integer >= 2. used values >= 2 model use polynomials continuous variables (.e. used predictors argument). Default 0. inter_order integer >= 0. interaction order explanatory variables. Default 0. control_gamlss function. control parameters outer iterations algorithm gamlss See gamlss.control documentation details. Default gamlss.control() verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"list object : model: \"gamlss\" class object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Linear Models — fit_abund_glm","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions family_selector(data = some_sp, response = \"ind_ha\") %>% tail()  # Fit a GLM model glm_1 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 0,   inter_order = 0,   predict_part = TRUE )  glm_1  # Using second order polynomials and first order interaction terms glm_2 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 2,   inter_order = 1,   predict_part = TRUE )  glm_2  # Using third order polynomials and second order interaction terms glm_3 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   poly = 3,   inter_order = 2,   predict_part = TRUE )  glm_3  # Setting formulas for different distribution parameters glm_4 <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAGA\",   fit_formula = ind_ha ~ bio12 + elevation + sand + eco,   sigma_formula = ind_ha ~ bio12 + elevation + sand,   poly = 0,   inter_order = 0,   predict_part = TRUE )  glm_4 } # }"},{"path":"/reference/fit_abund_net.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Artificial Neural Network models — fit_abund_net","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"Fit validate Artificial Neural Network models","code":""},{"path":"/reference/fit_abund_net.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"","code":"fit_abund_net(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   size,   decay = 0,   verbose = TRUE )"},{"path":"/reference/fit_abund_net.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. size numerical. size hidden layer. decay numerial. Value weight decay. Default 0. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_net.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_net.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Artificial Neural Network models — fit_abund_net","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a NET model mnet <- fit_abund_net(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   size = 32,   decay = 0.1,   predict_part = TRUE )  mnet } # }"},{"path":"/reference/fit_abund_raf.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Random Forests models — fit_abund_raf","title":"Fit and validate Random Forests models — fit_abund_raf","text":"Fit validate Random Forests models","code":""},{"path":"/reference/fit_abund_raf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Random Forests models — fit_abund_raf","text":"","code":"fit_abund_raf(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   mtry = length(c(predictors, predictors_f))/3,   ntree = 500,   verbose = TRUE )"},{"path":"/reference/fit_abund_raf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Random Forests models — fit_abund_raf","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. mtry numeric. Number variables randomly sampled candidates split. Default (length(c(predictors, predictors_f))/3) ntree numeric. Number trees grow. set small number, ensure every input row gets predicted least times. Default 500 verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_raf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Random Forests models — fit_abund_raf","text":"list object : model: \"randomForest\" class object randomForest package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":[]},{"path":"/reference/fit_abund_raf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Random Forests models — fit_abund_raf","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a RAF model mraf <- fit_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   mtry = 3,   ntree = 500,   predict_part = TRUE )  mraf } # }"},{"path":"/reference/fit_abund_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Support Vector Machine models — fit_abund_svm","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"Fit validate Support Vector Machine models","code":""},{"path":"/reference/fit_abund_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"","code":"fit_abund_svm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   kernel = \"rbfdot\",   sigma = \"automatic\",   C = 1,   verbose = TRUE )"},{"path":"/reference/fit_abund_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. kernel character. string defining kernel used algorithm. Default \"rbfdot\". sigma numeric character. Either \"automatic\" (recommended) inverse kernel width Radial Basis kernel function \"rbfdot\" Laplacian kernel \"laplacedot\". Default \"automatic\". C numeric. Cost constraints violation. Default 1. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/fit_abund_svm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_svm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Support Vector Machine models — fit_abund_svm","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a SVM model msvm <- fit_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   kernel = \"rbfdot\",   sigma = \"automatic\",   C = 1,   predict_part = TRUE )  msvm } # }"},{"path":"/reference/fit_abund_xgb.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"Fit validate Extreme Gradient Boosting models","code":""},{"path":"/reference/fit_abund_xgb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"","code":"fit_abund_xgb(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   nrounds = 100,   max_depth = 5,   eta = 0.1,   gamma = 1,   colsample_bytree = 1,   min_child_weight = 1,   subsample = 0.5,   objective = \"reg:squarederror\",   verbose = TRUE )"},{"path":"/reference/fit_abund_xgb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE. nrounds integer. Max number boosting iterations. Default 100. max_depth integer. maximum depth tree. Default 5 eta numeric. learning rate algorithm. Default 0.1 gamma numeric. Minimum loss reduction required make partition leaf node tree. Default 1. colsample_bytree numeric. Subsample ratio columns constructing tree. Default 1. min_child_weight numeric. Minimum sum instance weight needed child. Default 1. subsample numeric. Subsample ratio training instance. Default 0.5. objective character. learning task corresponding learning objective. Default \"reg:squarederror\", regression squared loss. verbose logical. FALSE, disables console messages. Default TRUE.","code":""},{"path":"/reference/fit_abund_xgb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"list object : model: \"xgb.Booster\" class object xgboost package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: Averaged performance metrics (see adm_eval). performance_part: Performance metrics replica partition. predicted_part: Observed predicted abundance test partition.","code":""},{"path":"/reference/fit_abund_xgb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Extreme Gradient Boosting models — fit_abund_xgb","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Extract data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore reponse variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Fit a XGB model mxgb <- fit_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = NULL,   partition = \".part\",   nrounds = 200,   max_depth = 5,   eta = 0.1,   gamma = 1,   colsample_bytree = 0.7,   min_child_weight = 2,   subsample = 0.3,   objective = \"reg:squarederror\",   predict_part = TRUE )  mxgb } # }"},{"path":"/reference/generate_arch_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"function generates list architectures either Deep Neural Netwokd (DNN) Convolutional Neural Network (CNN).","code":""},{"path":"/reference/generate_arch_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"","code":"generate_arch_list(   type,   number_of_features,   number_of_outputs,   n_layers = c(1, 2),   n_neurons = c(7),   sample_size = c(11, 11),   number_of_fc_layers = 1,   fc_layers_size = c(14),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   pooling = NULL,   batch_norm = TRUE,   dropout = 0 )"},{"path":"/reference/generate_arch_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"type string. Specifies type network. valid inputs \"dnn\" \"cnn\". number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. n_layers numeric. Vector specifies number layers networks. Default value 1 2. n_neurons vector. Specifies number neurons layer. Default  7. sample_size vector. Specifies size. Default c(11, 11) number_of_fc_layers numeric. Specifies number fully connected layers. Default 1. fc_layers_size vector. Specifies size fully connected layers. Default 14. conv_layers_kernel numeric. Specifies kernel size layers. Default 3. conv_layers_stride numeric. Specifies stride convolutional layers. Default 1. conv_layers_padding numeric. Specifies padding convolutional layers. Default 0. pooling numeric. Specifies 2D average pooling kernel size. Default NULL batch_norm logical. Specifies whether batch normalization included architecture. Default TRUE. dropout numeric. Default 0.","code":""},{"path":"/reference/generate_arch_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"list containing: arch_list: list generated architectures. arch_dict: list architecture dictionaries.","code":""},{"path":[]},{"path":"/reference/generate_arch_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architecture list for Deep Neural Network and Convolutional Neural Network — generate_arch_list","text":"","code":"if (FALSE) { # \\dontrun{ # Generating architectures for DNN, using batch normalization and dropout dnn_archs <- generate_arch_list(   type = \"dnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = c(2, 3, 4),   n_neurons = c(8, 16, 32, 64),   batch_norm = TRUE,   dropout = 0.2 )  dnn_archs$arch_dict # Matrices describing the networks length(dnn_archs$arch_list) # Generated 336 DNN architectures  # Generating architectures for CNN, using batch normalization, dropout and average pooling # Note that arguments meaning change with the context  cnn_archs <- generate_arch_list(   type = \"cnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = c(2, 3, 4), # now convolutional layers   n_neurons = c(8, 16, 32, 64),   sample_size = c(11, 11),   number_of_fc_layers = c(2, 4), # fully connected layers   fc_layers_size = c(16, 8),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   pooling = 1,   batch_norm = TRUE,   dropout = 0.2 )  cnn_archs$arch_dict # Matrices describing the networks length(cnn_archs$arch_list) # Generated 6720 CNN architectures  # The list size can be easily and greatly reduced with select_arch_list  dnn_archs_redux <- dnn_archs %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(dnn_archs_redux$arch_list) # from 336 to 29 architectures  cnn_archs_redux <- cnn_archs %>% select_arch_list(   type = c(\"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(cnn_archs_redux$arch_list) # from 6720 to 77 architectures } # }"},{"path":"/reference/generate_cnn_architecture.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"Generate architectures Convolutional Neural Network","code":""},{"path":"/reference/generate_cnn_architecture.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"","code":"generate_cnn_architecture(   number_of_features = 7,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = FALSE )"},{"path":"/reference/generate_cnn_architecture.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. sample_size vector. Specifies size. Default c(11, 11) number_of_conv_layers numeric. Specifies number convolutional layers. Default 2. conv_layers_size numeric. size convolutional layers. Default c(14, 28). conv_layers_kernel numeric. Specifies kernel size layers. Default 3. conv_layers_stride numeric. Specifies stride convolutional layers. Default 1. conv_layers_padding numeric. Specifies padding convolutional layers. Default 0. number_of_fc_layers numeric. Specifies number fully connected layers. Default 1. fc_layers_size vector. Specifies size fully connected layers. Default 14. pooling numeric. Specifies 2D average pooling kernel size. Default NULL batch_norm logical. Specifies whether batch normalization included architecture. Default TRUE. dropout numeric. Default 0. verbose logical. Specifies whether architecture printed. Default FALSE.","code":""},{"path":"/reference/generate_cnn_architecture.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"list containing: net: instantiated torch neural net. arch: string R expression instantiate neural network. arch_dict: list matrix describing architecture structure.","code":""},{"path":[]},{"path":"/reference/generate_cnn_architecture.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architectures for Convolutional Neural Network — generate_cnn_architecture","text":"","code":"if (FALSE) { # \\dontrun{ # Generate a Conclutional Neural Network with: cnn_arch <- generate_cnn_architecture(   number_of_features = 7, # seven input variables   number_of_outputs = 1, # one output   sample_size = c(11, 11), # image dimensions   number_of_conv_layers = 2, # two convolutional layers between input and output   conv_layers_size = c(14, 28), # of this size, respectively   conv_layers_kernel = 3, # with a 3 pixels kernel   conv_layers_stride = 1, # walking 1 pixel at a time   conv_layers_padding = 1, # with 1 pixel of padding   number_of_fc_layers = 1, # followed by one fully connected layer   fc_layers_size = c(28), # with 28 neurons   pooling = NULL, # without average pooling   batch_norm = TRUE, # with batch normalization   dropout = 0 # and without dropout )  cnn_arch$net() # a torch net cnn_arch$arch %>% cat() # the torch code to create it cnn_arch$arch_dict # and a quick description of its structure } # }"},{"path":"/reference/generate_dnn_architecture.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate architectures for Deep Neural Network — generate_dnn_architecture","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"Generate architectures Deep Neural Network","code":""},{"path":"/reference/generate_dnn_architecture.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"","code":"generate_dnn_architecture(   number_of_features = 7,   number_of_outputs = 1,   number_of_hidden_layers = 2,   hidden_layers_size = c(14, 7),   batch_norm = TRUE,   dropout = 0,   verbose = FALSE )"},{"path":"/reference/generate_dnn_architecture.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"number_of_features numeric. Value specifies number features dataset. number_of_outputs numeric. Value specifies number outputs. number_of_hidden_layers numeric. Number hidden layers neural network. Default 2. hidden_layers_size numeric vector. Size hidden layer neural network. Default c(14, 7). batch_norm logical. Whether include batch normalization layers. Default TRUE. dropout logical. Specifies whether dropout included architecture. Default FALSE. verbose logical. Whether print architecture. Default FALSE.","code":""},{"path":"/reference/generate_dnn_architecture.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"list containing: net: instantiated torch neural net. arch: string R expression instantiate neural network. arch_dict: list matrix describing architecture structure.","code":""},{"path":[]},{"path":"/reference/generate_dnn_architecture.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate architectures for Deep Neural Network — generate_dnn_architecture","text":"","code":"if (FALSE) { # \\dontrun{ # Generate a Deep Neural Network with: dnn_arch <- generate_dnn_architecture(   number_of_features = 8, # eight input variables   number_of_outputs = 1, # one output   number_of_hidden_layers = 5, # five layers between input and output   hidden_layers_size = c(8, 16, 32, 16, 8), # of this size, respectively   batch_norm = TRUE, # with batch normalization   dropout = 0, # without dropout )  dnn_arch$net() # a torch net dnn_arch$arch %>% cat() # the torch code to create it dnn_arch$arch_dict # and a quick description of its structure } # }"},{"path":"/reference/model_selection.html","id":null,"dir":"Reference","previous_headings":"","what":"Best hyperparameter selection — model_selection","title":"Best hyperparameter selection — model_selection","text":"Best hyperparameter selection","code":""},{"path":"/reference/model_selection.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Best hyperparameter selection — model_selection","text":"","code":"model_selection(hyper_combinations, metrics)"},{"path":"/reference/model_selection.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Best hyperparameter selection — model_selection","text":"hyper_combinations tibble data.frame. hyperparameter combinations performance metrics metrics character. performance metrics considered","code":""},{"path":"/reference/model_selection.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Best hyperparameter selection — model_selection","text":"list containing: optimal_combination: tibble optimal hyperparameter combination. all_combinations: combinations.","code":""},{"path":"/reference/p_abund_bpdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"Create bivariate partial dependence plots explore marginal effect predictors modeled abundance","code":""},{"path":"/reference/p_abund_bpdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"","code":"p_abund_bpdp(   model,   predictors = NULL,   resolution = 50,   training_data = NULL,   projection_data = NULL,   training_boundaries = NULL,   invert_transform = NULL,   response_name = NULL,   color_gradient = c(\"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\",     \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\"),   color_training_boundaries = \"white\",   set_max = NULL,   set_min = NULL,   theme = ggplot2::theme_classic() )"},{"path":"/reference/p_abund_bpdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"model model object found first element list returned function fit_abund_ tune_abund_ function families predictors character. Vector predictor name(s) calculate partial dependence plots. NULL predictors used. Default NULL resolution numeric. Number equally spaced points predict abundance values continuous predictors. Default 50 training_data data.frame tibble. Database response predictor values used fit model. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL training_boundaries character. Plot training conditions boundaries based training data. training_boundaries = \"convexh\", function delimit training environmental region based convex-hull. training_boundaries = \"rectangle\", function delimit training environmental region based four straight lines. used methods necessary provide data training_data argument.NULL predictors used. Default NULL. invert_transform logical. Invert transformation response variable. Useful cases response variable transformed one method adm_transform. Default NULL response_name character. Name response variable. Default NULL color_gradient character. Vector gradient colors. Default c(\"#000004\", \"#1B0A40\", \"#4A0C69\", \"#781B6C\", \"#A42C5F\", \"#CD4345\", \"#EC6824\", \"#FA990B\", \"#F7CF3D\", \"#FCFFA4\") color_training_boundaries character. vector one color used color points residuals, Default \"white\" set_max numeric. Set maximum abundance value plot set_min numeric. Set minimum abundance value plot theme ggplot2 theme. Default ggplot2::theme_classic()","code":""},{"path":"/reference/p_abund_bpdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"ggplot object","code":""},{"path":[]},{"path":"/reference/p_abund_bpdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bivariate partial dependence plots for abundance-based distribution models — p_abund_bpdp","text":"","code":"if (FALSE) { # \\dontrun{ require(terra) require(dplyr)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast() data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Bivariate Dependence Plots: # In different resolutions p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   resolution = 50 )  p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   resolution = 25 )  # With projection and training boundaries p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"elevation\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"rectangle\" )  p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"elevation\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"convexh\" )  # Customize colors and theme p_abund_bpdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   projection_data = envar,   training_boundaries = \"convexh\",   color_gradient =     c(       \"#122414\", \"#183C26\", \"#185437\", \"#106D43\", \"#0F874C\",       \"#2D9F54\", \"#61B463\", \"#8DC982\", \"#B3E0A7\", \"#D7F9D0\"     ),   color_training_boundaries = \"purple\",   theme = ggplot2::theme_dark() ) } # }"},{"path":"/reference/p_abund_pdp.html","id":null,"dir":"Reference","previous_headings":"","what":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"Create partial dependence plots explore marginal effect predictors modeled abundance","code":""},{"path":"/reference/p_abund_pdp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"","code":"p_abund_pdp(   model,   predictors = NULL,   resolution = 100,   resid = FALSE,   training_data = NULL,   invert_transform = NULL,   response_name = NULL,   projection_data = NULL,   rug = FALSE,   colorl = c(\"#462777\", \"#6DCC57\"),   colorp = \"black\",   alpha = 0.2,   theme = ggplot2::theme_classic() )"},{"path":"/reference/p_abund_pdp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"model model object found first element list returned function fit_abund_ tune_abund_ function families predictors character. Vector predictor name(s) calculate partial dependence plots. NULL predictors used. Default NULL resolution numeric. Number equally spaced points predict abundance values continuous predictors. Default 50 resid logical. Calculate residuals based training data. Default FALSE training_data data.frame tibble. Database response predictor values used fit model. Default NULL invert_transform logical. TRUE, inverse transformation response variable applied. response_name character. Name response variable. Default NULL projection_data SpatRaster. Raster layer environmental variables used model projection. argument used, function calculate partial dependence curves distinguishing conditions used training projection conditions (.e., projection data present projection area training). Default NULL rug logical. Add rug plot partial dependence plot. Default FALSE colorl character. Vector colors plot partial dependence curves. Default c(\"#462777\", \"#6DCC57\") colorp character. Color plot residuals. Default \"black\" alpha numeric. Transparency residuals. Default 0.2 theme ggplot2 theme. Default ggplot2::theme_classic()","code":""},{"path":"/reference/p_abund_pdp.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"ggplot object","code":""},{"path":"/reference/p_abund_pdp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"function creates partial dependent plots explore marginal effect predictors modeled abundance. projection_data used, function extract minimum maximum values found region time period model projected. range projection data greater training data plotted different color. Partial dependence plot used interpret model explore model may extrapolate outside environmental conditions used train model.","code":""},{"path":[]},{"path":"/reference/p_abund_pdp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Partial dependent plots for abundance-based distribution models — p_abund_pdp","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(terra)  # Load data envar <- system.file(\"external/envar.tif\", package = \"adm\") %>%   rast()  data(\"sppabund\") some_sp <- sppabund %>%   filter(species == \"Species one\")  # Fit some models mglm <- fit_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   distribution = \"ZAIG\",   poly = 3,   inter_order = 0,   predict_part = TRUE )  # Partial Dependence Plots:  # In different resolutions p_abund_pdp(   model = mglm,   resolution = 50,   training_data = some_sp,   response_name = \"Abundance\" )  p_abund_pdp(   model = mglm,   resolution = 5,   training_data = some_sp,   response_name = \"Abundance\" )  # Especific variables and different resulotions p_abund_pdp(   model = mglm,   predictors = c(\"bio12\", \"sand\"),   training_data = some_sp,   response_name = \"Abundance\" )  # With residuals and rug plot p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   resid = TRUE )  p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   rug = TRUE )  p_abund_pdp(   model = mglm,   training_data = some_sp,   response_name = \"Abundance\",   resid = TRUE,   rug = TRUE )  # Partial depence plot for training and projection condition found in a projection area p_abund_pdp(   model = mglm,   training_data = some_sp,   projection_data = envar,   response_name = \"Abundance\",   rug = TRUE )  # Custumize colors and theme p_abund_pdp(   model = mglm,   predictors = NULL,   resolution = 100,   resid = TRUE,   training_data = some_sp,   projection_data = envar,   colorl = c(\"blue\", \"red\"),   colorp = \"darkgray\",   alpha = 0.4,   theme = ggplot2::theme_dark() ) } # }"},{"path":"/reference/res_calculate.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate the output resolution of a layer — res_calculate","title":"Calculate the output resolution of a layer — res_calculate","text":"Calculate output resolution layer pooling operation Convolutional Neural Network.","code":""},{"path":"/reference/res_calculate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate the output resolution of a layer — res_calculate","text":"","code":"res_calculate(   type = c(\"layer\", \"pooling\"),   in_res,   kernel_size,   stride,   padding )"},{"path":"/reference/res_calculate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate the output resolution of a layer — res_calculate","text":"type string. Accepted values \"layer\" \"pooling\". in_res integer. represents resolution input layer. kernel_size integer. refers size kernel used convolution pooling operation. stride integer. stride length convolution pooling operation. used type \"layer\". padding integer. amount padding added input layer. used type \"layer\"","code":""},{"path":"/reference/res_calculate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate the output resolution of a layer — res_calculate","text":"function returns integer output resolution.","code":""},{"path":"/reference/res_calculate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate the output resolution of a layer — res_calculate","text":"type \"layer\" output resolution calculated ((in_res - kernel_size + (2 * padding)) / stride) + 1. type \"pooling\", output resolution calculated floor division in_res kernel_size.","code":""},{"path":"/reference/res_calculate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate the output resolution of a layer — res_calculate","text":"","code":"if (FALSE) { # \\dontrun{  # Calculating output resolution for a convolution layer res_calculate(type = \"layer\", in_res = 12, kernel_size = 2, stride = 2, padding = 0)  # Calculating output resolution for a pooling layer res_calculate(type = \"pooling\", in_res = 12, kernel_size = 2) } # }"},{"path":"/reference/select_arch_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"Select architectures Convolutional Neural Network Deep Neural Network","code":""},{"path":"/reference/select_arch_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"","code":"select_arch_list(   arch_list,   type = c(\"dnn\", \"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )"},{"path":"/reference/select_arch_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"arch_list list. Containing Convolutional Neural Network Deep Neural Network architectures. type character. Indicating type network. Options \"dnn\" \"cnn\". method character. Indicating method select architectures. Default \"percentile\". n_samples integer. Specifying number samples select per group. Default 1. min_max logical. TRUE, include networks minimal maximal parameters.","code":""},{"path":"/reference/select_arch_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"list new architecture list, dictionary, changes made.","code":""},{"path":"/reference/select_arch_list.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Select architectures for Convolutional Neural Network or Deep Neural Network — select_arch_list","text":"","code":"if (FALSE) { # \\dontrun{ # Generate some big list of architectures combining all argument values big_arch_list <- generate_arch_list(   type = \"dnn\",   number_of_features = 4,   number_of_outputs = 1,   n_layers = seq(from = 2, to = 6, by = 1),   n_neurons = c(8, 16, 32, 64) )  length(big_arch_list$arch_list) # 5456 architectures!  # It can be reduced sampling network architectures by its parameters number  reduced_arch_list <- big_arch_list %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1, # Keep at least one of each deepness   min_max = TRUE # Keep the network with the minimum and maximum number of parameters )  length(reduced_arch_list$arch_list) # from 5456 to 92 architectures!!  # See architectures names, deepness and number of parameters reduced_arch_list$changes } # }"},{"path":"/reference/sppabund.html","id":null,"dir":"Reference","previous_headings":"","what":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"data set containing species abundance three species, partition folds, environmental variables.","code":""},{"path":"/reference/sppabund.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"","code":"sppabund"},{"path":"/reference/sppabund.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"tibble 2767 rows 12 variables: species species names ind_ha species abundance expressed individuals per hectare x longitude species occurrences y latitude species occurrences bio1 bioclimatic variable related annual mean temperature bio3 bioclimatic variable related isothermality bio12 bioclimatic variable related annual precipitation bio15 bioclimatic variable related precipitation seasonality cfvo edaphic variable related volumetric fraction coarse fragments elevation topographic elevation sand edaphic variable related soil sand content eoc ecoregion .part1 ... .part3 repeate k-folds","code":""},{"path":"/reference/sppabund.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"A data set containing species abundance of three species, partition folds, and environmental variables. — sppabund","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) data(\"sppabund\") sppabund } # }"},{"path":"/reference/tune_abund_cnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"Fit validate Convolutional Neural Network exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_cnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"","code":"tune_abund_cnn(   data,   response,   predictors,   predictors_f = NULL,   x,   y,   rasters,   sample_size,   partition,   predict_part = FALSE,   grid = NULL,   architectures = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_cnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") x character. Column name longitude data. y character. Column name latitude data. rasters character. Path raster file environmental variables. sample_size numeric. dimension, pixels, raster samples. See cnn_make_samples beforehand. Default c(11,11) partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"batch_size\", \"n_epochs\", \"learning_rate\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: batch_size = 2^seq(4, 6) n_epochs = 10 learning_rate = seq(= 0.1, = 0.2, = 0.1) validation_patience = 2 fitting_patience = 5. case one hyperparameters provided, function complete grid default values. architectures list character. list object containing list architectures (nn_modules_generators torch), called \"arch_list\", list matrices describing architecture, called (\"arch_dict\"); use generate_arch_list function create . also possible use \"fit_intern\", construct default neural network architecture fit_abund_cnn. NULL, list architectures generated. Default NULL metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_cnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"/reference/tune_abund_cnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Convolutional Neural Network with exploration of hyper-parameters that optimize performance — tune_abund_cnn","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate some architectures many_archs <- generate_arch_list(   type = \"cnn\",   number_of_features = 3,   number_of_outputs = 1,   n_layers = c(2, 3),   n_neurons = c(16, 32),   sample_size = c(11, 11),   number_of_fc_layers = c(1), # fully connected layers   fc_layers_size = c(16),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   batch_norm = TRUE ) %>% select_arch_list(   type = c(\"cnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )  # Create a grid # Obs.: the grid is tested with every architecture, thus it can get very large. cnn_grid <- expand.grid(   learning_rate = c(0.01, 0.005),   n_epochs = c(50, 100),   batch_size = c(32),   validation_patience = c(2, 4),   fitting_patience = c(2, 4),   stringsAsFactors = FALSE )  # Tune a cnn model tuned_cnn <- tune_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = cnn_grid,   rasters = system.file(\"external/envar.tif\", package = \"adm\"),   x = \"x\",   y = \"y\",   sample_size = c(11, 11),   architectures = many_archs,   n_cores = 3 ) tuned_cnn  # It is also possible to use a only one architecture one_arch <- generate_cnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   sample_size = c(11, 11),   number_of_conv_layers = 2,   conv_layers_size = c(14, 28),   conv_layers_kernel = 3,   conv_layers_stride = 1,   conv_layers_padding = 0,   number_of_fc_layers = 1,   fc_layers_size = c(28),   pooling = NULL,   batch_norm = TRUE,   dropout = 0,   verbose = T )  tuned_cnn_2 <- tune_abund_cnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = cnn_grid,   architectures = one_arch,   rasters = system.file(\"external/envar.tif\", package = \"adm\"),   x = \"x\",   y = \"y\",   sample_size = c(11, 11),   n_cores = 3 )  tuned_cnn_2 } # }"},{"path":"/reference/tune_abund_dnn.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"Fit validate Deep Neural Network model exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_dnn.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"","code":"tune_abund_dnn(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   grid = NULL,   architectures = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_dnn.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"batch_size\", \"n_epochs\", \"learning_rate\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: batch_size = 2^seq(4, 6) n_epochs = 10 learning_rate = seq(= 0.1, = 0.2, = 0.1) validation_patience = 2 fitting_patience = 5. case one hyperparameters provided, function complete grid default values. architectures list character. list object containing list architectures (nn_modules_generators torch), called \"arch_list\", list matrices describing architecture, called (\"arch_dict\"); use generate_arch_list function create . also possible use \"fit_intern\", construct default neural network architecture fit_abund_dnn. NULL, list architectures generated. Default NULL metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_dnn.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"list object : model: \"luz_module_fitted\" object luz (torch framework). object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"/reference/tune_abund_dnn.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Deep Neural Network model with exploration of hyper-parameters that optimize performance — tune_abund_dnn","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Generate some architectures many_archs <- generate_arch_list(   type = \"dnn\",   number_of_features = 3,   number_of_outputs = 1,   n_layers = c(2, 3),   n_neurons = c(6, 12, 16) ) %>% select_arch_list(   type = c(\"dnn\"),   method = \"percentile\",   n_samples = 1,   min_max = TRUE )  # Create a grid # Obs.: the grid is tested with every architecture, thus it can get very large. dnn_grid <- expand.grid(   learning_rate = c(0.01, 0.005),   n_epochs = c(50, 100),   batch_size = c(32),   validation_patience = c(2, 4),   fitting_patience = c(2, 4),   stringsAsFactors = FALSE )  # Tune a DNN model tuned_dnn <- tune_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = dnn_grid,   architectures = many_archs,   n_cores = 3 )  tuned_dnn  # It is also possible to use a only one architecture one_arch <- generate_dnn_architecture(   number_of_features = 3,   number_of_outputs = 1,   number_of_hidden_layers = 3,   hidden_layers_size = c(8, 16, 8),   batch_norm = TRUE )  tuned_dnn_2 <- tune_abund_dnn(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = dnn_grid,   architectures = one_arch,   n_cores = 3 )  tuned_dnn_2 } # }"},{"path":"/reference/tune_abund_gam.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"Fit validate Generalized Additive Models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_gam.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"","code":"tune_abund_gam(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_gam.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe 'distribution', 'inter' columns values combinations rows. grid provided, function create default grid combining next hyperparameters: distribution = families selected family_selector, inter = \"automatic\". case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\") n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_gam.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"list object : model: \"gamlss\" object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_gam.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Additive Models with exploration of hyper-parameters that optimize performance — tune_abund_gam","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\") # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3) # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist() # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2) # Explore different family distributions suitable_distributions <- family_selector(data = some_sp, response = \"ind_ha\") suitable_distributions # Create a grid gam_grid <- expand.grid(   inter = \"automatic\",   distribution = suitable_distributions$family_call,   stringsAsFactors = FALSE ) # Tune a GAM model tuned_gam <- tune_abund_gam(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   fit_formula = formula(\"ind_ha ~ bio12 + elevation + sand + eco\"),   sigma_formula = formula(\"ind_ha ~ bio12 + elevation\"),   nu_formula = formula(\"ind_ha ~ bio12 + elevation\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = gam_grid,   n_cores = 3 )  tuned_gam } # }"},{"path":"/reference/tune_abund_gbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"Fit validate Generalized Boosted Regression models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_gbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"","code":"tune_abund_gbm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   distribution,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_gbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"n.trees\", \"interaction.depth\", \"n.minobsinnode\" \"shrinkage\" columns values rows. grid provided, function create default grid combining next hyperparameters: n.trees = c(100, 200, 300), interaction.depth = c(1, 2, 3), n.minobsinnode = c(5, 10, 15), shrinkage = seq(0.001, 0.1, = 0.05). case one hyperparameters provided, function complete grid default values. distribution character. string specifying distribution used. See gbm::gbm documentation details. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_gbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"list object : model: \"gbm\" object gbm package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance. selected_arch: numeric vector describing selected architecture layers.","code":""},{"path":"/reference/tune_abund_gbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Boosted Regression models with exploration of hyper-parameters that optimize performance — tune_abund_gbm","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid gbm_grid <- expand.grid(   interaction.depth = c(2, 4, 8, 16),   n.trees = c(100, 500, 1000),   n.minobsinnode = c(2, 5, 8),   shrinkage = c(0.1, 0.5, 0.7),    stringsAsFactors = FALSE )  tuned_gbm <- tune_abund_gbm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = gbm_grid,   distribution = \"gaussian\",   n_cores = 3 )  tuned_gbm } # }"},{"path":"/reference/tune_abund_glm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"Fit validate Generalized Linear Models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_glm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"","code":"tune_abund_glm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   sigma_formula = ~1,   nu_formula = ~1,   tau_formula = ~1,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_glm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL sigma_formula formula. formula fitting model nu parameter. Usage sigma_formula = ~ precipt + temp nu_formula formula. formula fitting model nu parameter. Usage nu_formula = ~ precipt + temp tau_formula formula. formula fitting model tau parameter. Usage tau_formula = ~ precipt + temp partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"distribution\", \"poly\", \"inter_order\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: poly = c(1, 2, 3), inter_order = c(0, 1, 2), distribution = families_hp$family_call. case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_glm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"list object : model: \"gamlss\" object gamlss package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_glm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Generalized Linear Models with exploration of hyper-parameters that optimize performance — tune_abund_glm","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) require(gamlss)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Explore different family distributions suitable_distributions <- family_selector(data = some_sp, response = \"ind_ha\") suitable_distributions  # Create a grid glm_grid <- expand.grid(   poly = c(2, 3),   inter_order = c(1, 2),   distribution = suitable_distributions$family_call,   stringsAsFactors = FALSE )  # Tune a GLM model tuned_glm <- tune_abund_glm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   fit_formula = formula(\"ind_ha ~ bio12 + elevation + sand + eco\"),   sigma_formula = formula(\"ind_ha ~ bio12 + elevation\"),   nu_formula = formula(\"ind_ha ~ bio12 + elevation\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = glm_grid,   n_cores = 3 )  tuned_glm } # }"},{"path":"/reference/tune_abund_net.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"Fit validate Shallow Neural Networks models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_net.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"","code":"tune_abund_net(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_net.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"size\" \"decay\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: size = seq(= length(c(predictors, predictors_f)), = 50, = 2), decay = seq(= 0, = 0.9, = 0.1). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_net.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"list object : model: \"nnet\" class object nnet package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_net.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Shallow Neural Networks models with exploration of hyper-parameters that optimize performance — tune_abund_net","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid net_grid <- expand.grid(   size = seq(from = 8, to = 32, by = 6),   decay = seq(from = 0, to = 0.4, by = 0.01),    stringsAsFactors = FALSE )  # Tune a NET model tuned_net <- tune_abund_net(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = net_grid,   n_cores = 3 )  tuned_net } # }"},{"path":"/reference/tune_abund_raf.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"Fit validate Random Forest models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_raf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"","code":"tune_abund_raf(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_raf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"mtry\" \"ntree\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: mtry = seq(2, length(predictors), = 1), ntree = seq(500, 1000, = 100). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_raf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"list object : model: \"randomForest\" class object randomForest package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_raf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Random Forest models with exploration of hyper-parameters that optimize performance — tune_abund_raf","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species two\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid raf_grid <- expand.grid(   mtry = seq(from = 2, to = 3, by = 1),   ntree = seq(from = 500, to = 1000, by = 100),   stringsAsFactors = FALSE )  # Tune a RAF model tuned_raf <- tune_abund_raf(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = raf_grid,   n_cores = 3 )  tuned_raf } # }"},{"path":"/reference/tune_abund_svm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"Fit validate Support Vector Machine models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_svm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"","code":"tune_abund_svm(   data,   response,   predictors,   predictors_f = NULL,   fit_formula = NULL,   partition,   predict_part = FALSE,   grid = NULL,   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_svm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") fit_formula formula. formula object response predictor variables (e.g. formula(abund ~ temp + precipt + sand + landform)). Note variables used must consistent used response, predictors, predictors_f arguments. Default NULL partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default FALSE. grid tibble data.frame. dataframe \"kernel\", \"sigma\", \"C\" columns values combinations rows. now grid provided, funcion create default grid combining next hyperparameters: C = seq(0.2, 1, = 0.2), sigma = \"automatic\", kernel = c(\"rbfdot\", \"laplacedot\"). case one hyperparameters provided, function complete grid default values. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_svm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"list object : model: \"ksvm\" class object kernlab package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_svm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Support Vector Machine models with exploration of hyper-parameters that optimize performance — tune_abund_svm","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr)  # Database with species abundance and x and y coordinates data(\"sppabund\")  # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species one\") %>%   dplyr::select(-.part2, -.part3)  # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist()  # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2)  # Create a grid svm_grid <- expand.grid(   sigma = \"automatic\",   C = c(0.5, 1, 2),   kernel = c(\"rbfdot\", \"laplacedot\"),   stringsAsFactors = FALSE )  # Tune a SVM model tuned_svm <- tune_abund_svm(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = svm_grid,   n_cores = 3 )  tuned_svm } # }"},{"path":"/reference/tune_abund_xgb.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"Fit validate Extreme Gradient Boosting models exploration hyper-parameters optimize performance","code":""},{"path":"/reference/tune_abund_xgb.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"","code":"tune_abund_xgb(   data,   response,   predictors,   predictors_f = NULL,   partition,   predict_part = FALSE,   grid = NULL,   objective = \"reg:squarederror\",   metrics = NULL,   n_cores = 1,   verbose = TRUE )"},{"path":"/reference/tune_abund_xgb.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"data tibble data.frame. Database response, predictors, partition values response character. Column name species abundance. predictors character. Vector column names quantitative predictor variables (.e. continuous variables). Usage predictors = c(\"temp\", \"precipt\", \"sand\") predictors_f character. Vector column names qualitative predictor variables (.e. ordinal nominal variables type). Usage predictors_f = c(\"landform\") partition character. Column name training validation partition groups. predict_part logical. Save predicted abundance testing data. Default = FALSE grid tibble data.frame. dataframe \"n.trees\", \"interaction.depth\", \"n.minobsinnode\" \"shrinkage\" columns values combinations rows. grid provided, function create default grid combining next hyperparameters: nrounds = c(100, 200, 300), max_depth = c(4, 6, 8), eta = c(0.2, 0.4, 0.5), gamma = c(1, 5, 10), colsample_bytree = c(0.5, 1, 2), min_child_weight = c(0.5, 1, 2), subsample = c(0.5, 0.75, 1). case one hyperparameters provided, function complete grid default values. objective character. learning task corresponding learning objective. Default \"reg:squarederror\", regression squared loss. metrics character. Vector one metrics c(\"corr_spear\",\"corr_pear\",\"mae\",\"pdisp\",\"inter\",\"slope\"). n_cores numeric. Number cores used parallel processing. verbose logical. FALSE, disables console messages. Default TRUE","code":""},{"path":"/reference/tune_abund_xgb.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"list object : model: \"xgb.Booster\" object xgboost package. object can used predicting. predictors: tibble quantitative (c column names) qualitative (f column names) variables use modeling. performance: tibble selected model's performance metrics calculated adm_eval. performance_part: tibble performance metrics test partition. predicted_part: tibble predicted abundance test partition. optimal_combination: tibble selected hyperparameter combination performance. all_combinations: tibble hyperparameters combinations performance.","code":""},{"path":"/reference/tune_abund_xgb.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit and validate Extreme Gradient Boosting models with exploration of hyper-parameters that optimize performance — tune_abund_xgb","text":"","code":"if (FALSE) { # \\dontrun{ require(dplyr) # Database with species abundance and x and y coordinates data(\"sppabund\") # Select data for a single species some_sp <- sppabund %>%   dplyr::filter(species == \"Species two\") %>%   dplyr::select(-.part2, -.part3) # Explore response variables some_sp$ind_ha %>% range() some_sp$ind_ha %>% hist() # Here we balance number of absences some_sp <-   balance_dataset(some_sp, response = \"ind_ha\", absence_ratio = 0.2) # Create a grid xgb_grid <- expand.grid(   nrounds = c(100, 300),   max_depth = c(4, 6, 8),   eta = c(0.2, 0.5),   gamma = c(1, 5, 10),   colsample_bytree = c(0.5, 1),   min_child_weight = c(0.5, 1, 2),   subsample = c(0.5, 1),    stringsAsFactors = FALSE ) # Tune a XGB model tuned_xgb <- tune_abund_xgb(   data = some_sp,   response = \"ind_ha\",   predictors = c(\"bio12\", \"elevation\", \"sand\"),   predictors_f = c(\"eco\"),   partition = \".part\",   predict_part = TRUE,   metrics = c(\"corr_pear\", \"mae\"),   grid = xgb_grid,   objective = \"reg:squarederror\",   n_cores = 3 ) tuned_xgb } # }"}]
